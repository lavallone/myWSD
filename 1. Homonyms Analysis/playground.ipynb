{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pAk0A9TK70"
      },
      "source": [
        "# **WSD** - paper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8uLXP5Wf4O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LKij-njNdvYg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 99\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# import stuffs\n",
        "from data_module import WSD_DataModule\n",
        "from hyperparameters import Hparams\n",
        "from train import train_model\n",
        "from model import WSD_Model\n",
        "from evaluation import base_evaluation, fine2cluster_evaluation, cluster_filter_evaluation\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# to have a better workflow using python notebooks\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    _ = pl.seed_everything(seed)\n",
        "set_seed(99)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLDtSF5Zl8o"
      },
      "source": [
        "## Look at the data \n",
        "[simple checks about data properties]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NiVoMDeZZl8r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for coarse-grained WSD is 106553\n"
          ]
        }
      ],
      "source": [
        "# TOTAL NUMBER OF SENSES for coarse-grained WSD\n",
        "hparams = asdict(Hparams()) # instantiate hyperparamters file\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = list(d.keys())\n",
        "print(f\"Length of sense inventory for coarse-grained WSD is {len(all_senses_list)}\") # with the old data was 2158"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wBDKkGb1Zl8r"
      },
      "outputs": [],
      "source": [
        "# Since we are dealing with neural networks we need to encode the sense invectory and simply create a mapping between \n",
        "# coarse-grained senses and indices.\n",
        "\n",
        "# let's build sense2id and id2sense map for coarse-grained senses\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/cluster_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/cluster_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1i75G_beZl8t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for fine-grained WSD is 154440\n",
            "Length of sense inventory for fine-grained WSD (with no duplicates) is 117659\n"
          ]
        }
      ],
      "source": [
        "# let's build sense2id and id2sense map for fine-graned senses\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = []\n",
        "for k in d.keys():\n",
        "    for fine_s in d[k]:\n",
        "        all_senses_list.append(fine_s[0])\n",
        "print(f\"Length of sense inventory for fine-grained WSD is {len(all_senses_list)}\") # with the old data was 4476\n",
        "# there could be that a fine sense is present in multiple clusters, not only one!\n",
        "# we need a set wth no duplicates!\n",
        "all_senses_list = list(set(all_senses_list))\n",
        "print(f\"Length of sense inventory for fine-grained WSD (with no duplicates) is {len(all_senses_list)}\")\n",
        "\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/fine_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/fine_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAlXfvU1Zl8t"
      },
      "outputs": [],
      "source": [
        "# Because of some approaches I'll develop later I need\n",
        "# to build a direct mapping between fine and coarse-grained (we already have the opposite mapping)\n",
        "\n",
        "# d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "# fine2coarse = {}\n",
        "# for k in d.keys():\n",
        "#     for fine_s in d[k]:\n",
        "#         fine2coarse[list(fine_s.keys())[0]] = k\n",
        "\n",
        "# json.dump(fine2coarse, open(\"data/map/fine2coarse.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how many <UNK> token we generate without any particular type of preprocessing!\n",
        "# data = WSD_DataModule(hparams)\n",
        "# data.setup()\n",
        "\n",
        "# tot_tokens = 0\n",
        "# tot_unk = 0\n",
        "# for batch in tqdm(data.train_dataloader()):\n",
        "#     for input in batch[\"inputs\"][\"input_ids\"]:\n",
        "#         for e in input:\n",
        "#             if e.item() == 0: # we reached <PAD> tokens\n",
        "#                 break\n",
        "#             tot_tokens+=1\n",
        "#             if e.item() == 100: # is the <UNK> token\n",
        "#                 tot_unk+=1\n",
        "# print(f\"We have a total of {tot_tokens} tokens\")\n",
        "# print(f\"with {tot_unk} <UNK> tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Some quick analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:02<5:04:22,  2.59s/it]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [03:09<00:00, 37.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 97.09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  7%|▋         | 1/15 [00:00<00:07,  1.76it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            " 13%|█▎        | 2/15 [00:00<00:04,  2.85it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:11<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 97.14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:39,  1.54it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 93.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of 1 cluster candidates (so that the model cannot make wrong predictions)\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\") # 97.09% is high!\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1\n",
        "            \n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1  \n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:50<00:00, 41.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  7%|▋         | 1/15 [00:00<00:09,  1.40it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:11<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:05,  2.34it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the average length of fine candidates\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"TRAIN - {round(tot_lenght/tot)}\") # 6.82\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"VAL - {round(tot_lenght/tot)}\")\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"TEST - {round(tot_lenght/tot)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:00<57:37,  2.04it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 4/7064 [00:00<16:27,  7.15it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:59<00:00, 39.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:04<00:00, 33.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of CLUSTER gold labels of lenght 1\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:00<1:13:13,  1.61it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 3/7064 [00:00<27:42,  4.25it/s]  Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:54<00:00, 40.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 99.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:01<00:00, 12.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 99.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:49,  1.39it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 87.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of FINE gold labels of lenght 1\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "multi_label_list = []\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'multi_label_list' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/lavallone/Desktop/Homonyms_WSD/homonym/notebook.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lavallone/Desktop/Homonyms_WSD/homonym/notebook.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lavallone/Desktop/Homonyms_WSD/homonym/notebook.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(multi_label_list))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lavallone/Desktop/Homonyms_WSD/homonym/notebook.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(multi_label_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lavallone/Desktop/Homonyms_WSD/homonym/notebook.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lengths \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m multi_label_list]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'multi_label_list' is not defined"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "print(len(multi_label_list))\n",
        "print(multi_label_list)\n",
        "lengths = [len(l) for l in multi_label_list]\n",
        "c = Counter(lengths)\n",
        "print(c)\n",
        "\n",
        "fine_id2sense = json.load(open(\"data/mapping/fine_id2sense.json\", \"r\"))\n",
        "multi_label_list = [fine_id2sense[str(e)] for l in multi_label_list for e in l]\n",
        "print(len(multi_label_list))\n",
        "print(multi_label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "___________ fine MODEL ___________\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'inputs': {'input_ids': tensor([[  101,  2353, 14125,   119,   127,  9378,   169,   169,  1109,  9190,\n",
            "          1233,  3341,  2008,   169,   169,  2752,  1106,  1844,  1118,  1366,\n",
            "          1105,  1565,  1104,  1412,  8304,  1115,  1108,  2103,  1107,  1103,\n",
            "         20456,   119,   129,  2486,  1104,  1103,  3603,  1104,  1103,  1237,\n",
            "          3875,  1791,   119,   102,     0,     0,     0,     0,     0],\n",
            "        [  101,  2353,  7640, 13806,  1195,  1125,  2751,  1115,  1103,   169,\n",
            "           169,  3981,  2612,   169,   169,  1104, 12501,  1757,  1110,  1106,\n",
            "          1129,  1276,  1107,  1103,  1415,  2849,  1104, 10883,  5178,  1105,\n",
            "          9556,   118,   170,  7441,  1158,  1234,  1107,  1103, 12501,  1416,\n",
            "           119,   102,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2353,  7640, 13806,  1195,  1125,  2751,  1115,  1103,   169,\n",
            "           169,  3981,  2612,   169,   169,  1104, 12501,  1757,  1110,  1106,\n",
            "          1129,  1276,  1107,  1103,  1415,  2849,  1104, 10883,  5178,  1105,\n",
            "          9556,   118,   170,  7441,  1158,  1234,  1107,  1103, 12501,  1416,\n",
            "           119,  1284,  1138,  1189,  1185,  1216,  4195,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1]])}, 'cluster_gold': [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 24898, -100, -100, 38460, -100, -100, -100, -100, -100, -100, -100, -100, -100, 85555, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, -100, 50109, 86045, -100, -100, 81809, -100, -100, -100, -100, 41054, -100, -100, -100, -100, -100, -100, -100, 102945, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 61705, -100, -100, 36438, -100, -100, -100, -100, -100, -100]], 'cluster_candidates': [[[24898], [38460], [85555]], [[50109], [86045], [81809], [41054], [102945]], [[61705], [36438]]], 'fine_gold': [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2497, -100, -100, 116871, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47414, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, -100, 81480, 114157, -100, -100, 70850, -100, -100, -100, -100, 91567, -100, -100, -100, -100, -100, -100, -100, 112329, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 105624, -100, -100, 72603, -100, -100, -100, -100, -100, -100]], 'fine_candidates': [[[2497, 106919, 110446, 33115, 40721, 108682, 112826], [116871, 76977], [62399, 47414, 58781, 72217, 58783, 108982]], [[81480, 53699, 66443], [114157, 87660, 76815, 98900, 16696], [112329, 70850, 43542, 45443, 8639, 36801, 62482, 22087], [91567, 106979, 84796, 110800, 63088], [1048, 112329, 90664, 40600, 71391, 37409, 106944, 43542, 45443, 46931, 41817, 95180, 66968, 109789, 112731, 835]], [[91944, 68248, 105288, 94216, 102519, 69031, 43805, 12167, 99922, 33109, 66176, 25867, 25625, 20465, 36453, 105624, 35163, 15917, 78955, 102340, 114936, 67471, 48833, 31152, 37635, 42958, 65504, 2359, 101042, 92573, 95904, 58696, 49007, 95608, 89262, 2689, 44699, 39085, 45298, 59587, 117616, 9311, 68501, 67234, 64284, 84183, 38500, 111555, 82677], [45439, 65136, 112677, 105217, 72603, 42398, 46562]]]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of FINE gold labels of lenght 1\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "model = WSD_Model(hparams)\n",
        "\n",
        "for b in data.val_dataloader():\n",
        "    #model(b)\n",
        "    print(b)\n",
        "    break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFGbs4_Zl8w"
      },
      "source": [
        "### Preprocessing\n",
        "[do not necessarily use it]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmMJ-O2Zl8x"
      },
      "source": [
        "#### Clean tokens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUBPDkLZl8x"
      },
      "source": [
        "With respect to the first homework the *cleaning* operations (also due the power of *BERT Tokenizer*) are very basic and not \"aggressive\".\n",
        "\n",
        "> 🔸 The function I implemented is \"*clean_tokens*\" from the *data_module.py* file. Of course, this function is applied to all the dataset splits (*train/val/test*)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfV9bgkZl8x"
      },
      "source": [
        "#### Filter sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jd3TTvLZZl8x"
      },
      "source": [
        "Another important step before finishing the preprocessing part, is to filter out the *training* sentences. This is something it has to be done only at training time because the test/val datasets don't have to be touched in this sense. <br> Let's first see which is the histogram of sentences length."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaynqYLrbhu9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C47XCTYUZl82"
      },
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaE5pz6fZl82"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"coarse\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMMZ7pSZl82"
      },
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ca5ZHIZl83"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"fine\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SwNRcdPbjvg"
      },
      "source": [
        "### Hparams tuning\n",
        "[if needed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fns1bRRPZl83"
      },
      "outputs": [],
      "source": [
        "# def training_pipeline(config=None):\n",
        "#     hparams_tuning = False\n",
        "#     version_name = \"BASELINE\"\n",
        "#     with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\", config=config):\n",
        "#         seed = wandb.config.seed if hparams_tuning else 1999\n",
        "#         set_seed(seed)\n",
        "#         hparams = asdict(Hparams())\n",
        "#         # when doing the hparams search, this is how each run we change them to search for the best combinations!\n",
        "#         if hparams_tuning:\n",
        "#             hparams[\"batch_size\"] = wandb.config.batch_size\n",
        "#             hparams[\"dropout\"] = wandb.config.dropout\n",
        "#             hparams[\"lr\"] = wandb.config.lr\n",
        "#             hparams[\"hidden_dim\"] = wandb.config.hidden_dim\n",
        "\n",
        "#         data = WSD_DataModule(hparams)\n",
        "#         model = WSD_Model(hparams,\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine2coarse.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_fine_defs_map.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_id2sense.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_id2sense.json\", \"r\"))\n",
        "#                           )\n",
        "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#         model.to(device)\n",
        "        \n",
        "#         train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAV15D1bl14"
      },
      "outputs": [],
      "source": [
        "# wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "# sweep_config = {'method': 'random',\n",
        "#                 'metric': {'goal': 'maximize', 'name': 'val_micro_f1', 'target' : 0.89},\n",
        "#                 'parameters': {\n",
        "#                                 'batch_size': {'values': [64, 128, 256, 512]},\n",
        "#                                 'dropout': {'distribution': 'uniform', 'min': 0.3, 'max': 0.5},\n",
        "#                                 'lr': {'distribution': 'uniform', 'min': 1e-5, 'max': 1e-2},\n",
        "#                                 'hidden_dim': {'distribution': 'int_uniform', 'min': 200, 'max': 600},\n",
        "#                             }\n",
        "#                }\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP\", entity=\"lavallone\")\n",
        "# wandb.agent(sweep_id, function=training_pipeline, count=20)\n",
        "# wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4txjgQRtGs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE2CLUSTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on homonym clusters using a fine-grained model\n",
        "fine2cluster_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLUSTER FILTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "best_fine_ckpt = \"checkpoints/fine.ckpt\"\n",
        "\n",
        "coarse_model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fine_model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(coarse_model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on fine senses using a coarse model for filtering out\n",
        "cluster_filter_evaluation(coarse_model, fine_model, data, oracle_or_not=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dzLDtSF5Zl8o",
        "ky0iP4FZZl8q",
        "KqwcyAGAZl8t",
        "lVFGbs4_Zl8w",
        "5FcrCHdjQ9Jf",
        "cTH0L6tvZl8z",
        "_fybkSIAZl8z",
        "ZEzTn9RuZl81",
        "IFPNDG-4Zl81",
        "BaynqYLrbhu9",
        "_SwNRcdPbjvg",
        "ue4txjgQRtGs",
        "Q3V_DSDzZl86"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
