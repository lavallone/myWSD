{"cells":[{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Top 20 predictions for [MASK]: ['team also then game always automatically only now usually player therefore never still instead again group had winner play,']\n"]}],"source":["from transformers import AutoTokenizer, BertForMaskedLM\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n","model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n","\n","inputs = tokenizer(\"Another of these advantages to which it is alluded to with the fare-play is to avoid that the team of the lawbreaker [MASK] takes advantage unjustly of the opposite team , which did not happen either .\", return_tensors=\"pt\")\n","\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","# retrieve index of [MASK]\n","mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n","\n","# Get top predictions\n","top_k = 20\n","top_indices = torch.topk(logits[0, mask_token_index], top_k).indices.tolist()\n","\n","# Convert indices back to tokens\n","predicted_tokens = [tokenizer.decode(index) for index in top_indices]\n","\n","print(\"Top\", top_k, \"predictions for [MASK]:\", predicted_tokens)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.275272011756897, 0.13427379727363586, 0.11213163286447525, 0.19085539877414703, 0.17880640923976898]\n"]}],"source":["candidates = [  \"a person who participates in or is skilled at some game\",\n","      \"someone who plays a musical instrument (as a profession)\",\n","      \"a theatrical performer\",\n","      \"a person who pursues a number of different social and sexual partners simultaneously\",\n","      \"an important participant (as in a business deal)\"]\n","\n","from sentence_transformers import SentenceTransformer\n","from torch.nn import CosineSimilarity\n","\n","# Load Sentence-BERT model\n","model_name = 'bert-base-nli-mean-tokens'\n","model_2 = SentenceTransformer(model_name)\n","\n","embeddings1 = model_2.encode(predicted_tokens, convert_to_tensor=True).reshape(1, -1)\n","\n","cos_sim_list = []\n","for sentence2 in candidates:\n","    embeddings2 = model_2.encode(sentence2, convert_to_tensor=True).reshape(1, -1)  \n","    cos_sim = CosineSimilarity(dim=1, eps=1e-6)\n","    similarity_score = cos_sim(embeddings1, embeddings2).item()\n","    cos_sim_list.append(similarity_score)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import json\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, BertForMaskedLM\n","import torch\n","from sentence_transformers import SentenceTransformer\n","from torch.nn import CosineSimilarity"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","100%|██████████| 7253/7253 [15:28<00:00,  7.81it/s]\n"]}],"source":["# FIRST ATTEMPT FOR A LOOP\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n","model = BertForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n","top_k = 10\n","model_name = 'bert-base-nli-mean-tokens'\n","model_2 = SentenceTransformer(model_name)\n","\n","with open(\"ALL_preprocessed.json\") as f:\n","    data = json.load(f)\n","    ris = []\n","    for i in tqdm(range(len(data))):\n","        input_sentence = data[i][\"text\"]\n","        masked_sentence = input_sentence.replace(data[i][\"word\"], \"[MASK]\")\n","        inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            logits = model(**inputs).logits\n","        # retrieve index of [MASK]\n","        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n","        # get top predictions\n","        top_indices = torch.topk(logits[0, mask_token_index], top_k).indices.tolist()\n","        # convert indices back to tokens\n","        predicted_tokens = [tokenizer.decode(index) for index in top_indices]\n","        \n","        candidates = data[i][\"definitions\"]\n","        embeddings1 = model_2.encode(predicted_tokens, convert_to_tensor=True).mean(axis=0)\n","        cos_sim_list = []\n","        for sentence2 in candidates:\n","            embeddings2 = model_2.encode([sentence2], convert_to_tensor=True).reshape(1,-1)\n","            cos_sim = CosineSimilarity(dim=1, eps=1e-6)\n","            similarity_score = cos_sim(embeddings1, embeddings2).item()\n","            cos_sim_list.append(similarity_score)\n","        best_candidate_idx = torch.tensor(cos_sim_list).argmax(axis=-1).item()\n","        ris.append( {\"id\" : data[i][\"id\"], \"answer\" : candidates[best_candidate_idx], \"gold\" : data[i][\"gold_definitions\"]} )\n","        "]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["| ACCURACY is: 0.5059975182683027 |\n"]}],"source":["eval_ris = 0\n","for e in ris:\n","    if e[\"answer\"] in e[\"gold\"]:\n","        eval_ris += 1\n","        \n","print(f\"| ACCURACY is: {eval_ris/len(ris)} |\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of NLP Notebook #1 - Introduction to colab, numpy and PyTorch.ipynb","provenance":[{"file_id":"1sk7hjLT6I442NQvCTm-69twM1ol7x_xh","timestamp":1646693453928},{"file_id":"1wjQkCKDZpNzTYEVAMsEQ60TdAQ8vgRKZ","timestamp":1582967606658}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
