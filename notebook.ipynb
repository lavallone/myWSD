{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pAk0A9TK70"
      },
      "source": [
        "# **WSD** - paper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8uLXP5Wf4O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKij-njNdvYg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lavallone/miniconda3/envs/sappia/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "Global seed set to 99\n"
          ]
        }
      ],
      "source": [
        "# import stuffs\n",
        "from data_module import WSD_DataModule\n",
        "from hyperparameters import Hparams\n",
        "from train import train_model\n",
        "from model import WSD_Model\n",
        "from evaluation import base_evaluation, fine2cluster_evaluation, cluster_filter_evaluation\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# to have a better workflow using python notebooks\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    _ = pl.seed_everything(seed)\n",
        "set_seed(99)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLDtSF5Zl8o"
      },
      "source": [
        "## Look at the data \n",
        "[simple checks about data properties]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NiVoMDeZZl8r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for coarse-grained WSD is 106553\n"
          ]
        }
      ],
      "source": [
        "# TOTAL NUMBER OF SENSES for coarse-grained WSD\n",
        "hparams = asdict(Hparams()) # instantiate hyperparamters file\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = list(d.keys())\n",
        "print(f\"Length of sense inventory for coarse-grained WSD is {len(all_senses_list)}\") # with the old data was 2158"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wBDKkGb1Zl8r"
      },
      "outputs": [],
      "source": [
        "# Since we are dealing with neural networks we need to encode the sense invectory and simply create a mapping between \n",
        "# coarse-grained senses and indices.\n",
        "\n",
        "# let's build sense2id and id2sense map for coarse-grained senses\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/cluster_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/cluster_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1i75G_beZl8t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for fine-grained WSD is 154440\n",
            "Length of sense inventory for fine-grained WSD (with no duplicates) is 117659\n"
          ]
        }
      ],
      "source": [
        "# let's build sense2id and id2sense map for fine-graned senses\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = []\n",
        "for k in d.keys():\n",
        "    for fine_s in d[k]:\n",
        "        all_senses_list.append(fine_s[0])\n",
        "print(f\"Length of sense inventory for fine-grained WSD is {len(all_senses_list)}\") # with the old data was 4476\n",
        "# there could be that a fine sense is present in multiple clusters, not only one!\n",
        "# we need a set wth no duplicates!\n",
        "all_senses_list = list(set(all_senses_list))\n",
        "print(f\"Length of sense inventory for fine-grained WSD (with no duplicates) is {len(all_senses_list)}\")\n",
        "\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/fine_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/fine_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAlXfvU1Zl8t"
      },
      "outputs": [],
      "source": [
        "# Because of some approaches I'll develop later I need\n",
        "# to build a direct mapping between fine and coarse-grained (we already have the opposite mapping)\n",
        "\n",
        "# d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "# fine2coarse = {}\n",
        "# for k in d.keys():\n",
        "#     for fine_s in d[k]:\n",
        "#         fine2coarse[list(fine_s.keys())[0]] = k\n",
        "\n",
        "# json.dump(fine2coarse, open(\"data/map/fine2coarse.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how many <UNK> token we generate without any particular type of preprocessing!\n",
        "# data = WSD_DataModule(hparams)\n",
        "# data.setup()\n",
        "\n",
        "# tot_tokens = 0\n",
        "# tot_unk = 0\n",
        "# for batch in tqdm(data.train_dataloader()):\n",
        "#     for input in batch[\"inputs\"][\"input_ids\"]:\n",
        "#         for e in input:\n",
        "#             if e.item() == 0: # we reached <PAD> tokens\n",
        "#                 break\n",
        "#             tot_tokens+=1\n",
        "#             if e.item() == 100: # is the <UNK> token\n",
        "#                 tot_unk+=1\n",
        "# print(f\"We have a total of {tot_tokens} tokens\")\n",
        "# print(f\"with {tot_unk} <UNK> tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Playground**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7064/7064 [02:30<00:00, 46.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97.09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of 1 cluster candidates (so that the model cannot make wrong predictions)\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1\n",
        "            \n",
        "print(round((c/tot)*100, 2)) # 97.09% is high!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to see the average length of fine candidates\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "            \n",
        "print(round(tot_lenght/tot)) # 6.82"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFGbs4_Zl8w"
      },
      "source": [
        "### Preprocessing\n",
        "[do not necessarily use it]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmMJ-O2Zl8x"
      },
      "source": [
        "#### Clean tokens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUBPDkLZl8x"
      },
      "source": [
        "With respect to the first homework the *cleaning* operations (also due the power of *BERT Tokenizer*) are very basic and not \"aggressive\".\n",
        "\n",
        "> ðŸ”¸ The function I implemented is \"*clean_tokens*\" from the *data_module.py* file. Of course, this function is applied to all the dataset splits (*train/val/test*)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfV9bgkZl8x"
      },
      "source": [
        "#### Filter sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jd3TTvLZZl8x"
      },
      "source": [
        "Another important step before finishing the preprocessing part, is to filter out the *training* sentences. This is something it has to be done only at training time because the test/val datasets don't have to be touched in this sense. <br> Let's first see which is the histogram of sentences length."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaynqYLrbhu9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C47XCTYUZl82"
      },
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaE5pz6fZl82"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"coarse\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMMZ7pSZl82"
      },
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ca5ZHIZl83"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"fine\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SwNRcdPbjvg"
      },
      "source": [
        "### Hparams tuning\n",
        "[if needed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fns1bRRPZl83"
      },
      "outputs": [],
      "source": [
        "# def training_pipeline(config=None):\n",
        "#     hparams_tuning = False\n",
        "#     version_name = \"BASELINE\"\n",
        "#     with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\", config=config):\n",
        "#         seed = wandb.config.seed if hparams_tuning else 1999\n",
        "#         set_seed(seed)\n",
        "#         hparams = asdict(Hparams())\n",
        "#         # when doing the hparams search, this is how each run we change them to search for the best combinations!\n",
        "#         if hparams_tuning:\n",
        "#             hparams[\"batch_size\"] = wandb.config.batch_size\n",
        "#             hparams[\"dropout\"] = wandb.config.dropout\n",
        "#             hparams[\"lr\"] = wandb.config.lr\n",
        "#             hparams[\"hidden_dim\"] = wandb.config.hidden_dim\n",
        "\n",
        "#         data = WSD_DataModule(hparams)\n",
        "#         model = WSD_Model(hparams,\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine2coarse.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_fine_defs_map.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_id2sense.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_id2sense.json\", \"r\"))\n",
        "#                           )\n",
        "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#         model.to(device)\n",
        "        \n",
        "#         train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAV15D1bl14"
      },
      "outputs": [],
      "source": [
        "# wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "# sweep_config = {'method': 'random',\n",
        "#                 'metric': {'goal': 'maximize', 'name': 'val_micro_f1', 'target' : 0.89},\n",
        "#                 'parameters': {\n",
        "#                                 'batch_size': {'values': [64, 128, 256, 512]},\n",
        "#                                 'dropout': {'distribution': 'uniform', 'min': 0.3, 'max': 0.5},\n",
        "#                                 'lr': {'distribution': 'uniform', 'min': 1e-5, 'max': 1e-2},\n",
        "#                                 'hidden_dim': {'distribution': 'int_uniform', 'min': 200, 'max': 600},\n",
        "#                             }\n",
        "#                }\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP\", entity=\"lavallone\")\n",
        "# wandb.agent(sweep_id, function=training_pipeline, count=20)\n",
        "# wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4txjgQRtGs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE2CLUSTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on homonym clusters using a fine-grained model\n",
        "fine2cluster_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLUSTER FILTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "best_fine_ckpt = \"checkpoints/fine.ckpt\"\n",
        "\n",
        "coarse_model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fine_model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(coarse_model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on fine senses using a coarse model for filtering out\n",
        "cluster_filter_evaluation(coarse_model, fine_model, data, oracle_or_not=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dzLDtSF5Zl8o",
        "ky0iP4FZZl8q",
        "KqwcyAGAZl8t",
        "lVFGbs4_Zl8w",
        "5FcrCHdjQ9Jf",
        "cTH0L6tvZl8z",
        "_fybkSIAZl8z",
        "ZEzTn9RuZl81",
        "IFPNDG-4Zl81",
        "BaynqYLrbhu9",
        "_SwNRcdPbjvg",
        "ue4txjgQRtGs",
        "Q3V_DSDzZl86"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
