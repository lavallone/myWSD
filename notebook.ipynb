{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S7pAk0A9TK70"
      },
      "source": [
        "# **WSD** - paper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8uLXP5Wf4O"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKij-njNdvYg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lavallone/miniconda3/envs/sappia/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n",
            "Global seed set to 99\n"
          ]
        }
      ],
      "source": [
        "# import stuffs\n",
        "from data_module import WSD_DataModule\n",
        "from hyperparameters import Hparams\n",
        "from train import train_model\n",
        "from model import WSD_Model\n",
        "from evaluation import base_evaluation, fine2cluster_evaluation, cluster_filter_evaluation\n",
        "\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# to have a better workflow using python notebooks\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# setting the seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    _ = pl.seed_everything(seed)\n",
        "set_seed(99)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLDtSF5Zl8o"
      },
      "source": [
        "## Look at the data \n",
        "[simple checks about data properties]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NiVoMDeZZl8r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for coarse-grained WSD is 106553\n"
          ]
        }
      ],
      "source": [
        "# TOTAL NUMBER OF SENSES for coarse-grained WSD\n",
        "hparams = asdict(Hparams()) # instantiate hyperparamters file\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = list(d.keys())\n",
        "print(f\"Length of sense inventory for coarse-grained WSD is {len(all_senses_list)}\") # with the old data was 2158"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wBDKkGb1Zl8r"
      },
      "outputs": [],
      "source": [
        "# Since we are dealing with neural networks we need to encode the sense invectory and simply create a mapping between \n",
        "# coarse-grained senses and indices.\n",
        "\n",
        "# let's build sense2id and id2sense map for coarse-grained senses\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/cluster_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/cluster_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1i75G_beZl8t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of sense inventory for fine-grained WSD is 154440\n",
            "Length of sense inventory for fine-grained WSD (with no duplicates) is 117659\n"
          ]
        }
      ],
      "source": [
        "# let's build sense2id and id2sense map for fine-graned senses\n",
        "d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "all_senses_list = []\n",
        "for k in d.keys():\n",
        "    for fine_s in d[k]:\n",
        "        all_senses_list.append(fine_s[0])\n",
        "print(f\"Length of sense inventory for fine-grained WSD is {len(all_senses_list)}\") # with the old data was 4476\n",
        "# there could be that a fine sense is present in multiple clusters, not only one!\n",
        "# we need a set wth no duplicates!\n",
        "all_senses_list = list(set(all_senses_list))\n",
        "print(f\"Length of sense inventory for fine-grained WSD (with no duplicates) is {len(all_senses_list)}\")\n",
        "\n",
        "sense2id = {}\n",
        "id2sense = {}\n",
        "\n",
        "idx=0\n",
        "for sense in all_senses_list:\n",
        "    sense2id[sense] = idx\n",
        "    id2sense[idx] = sense\n",
        "    idx+=1\n",
        "\n",
        "sense2id[\"<UNK>\"] = idx\n",
        "id2sense[idx] = \"<UNK>\"\n",
        "    \n",
        "json.dump(sense2id, open(\"data/mapping/fine_sense2id.json\", \"w\"))\n",
        "json.dump(id2sense, open(\"data/mapping/fine_id2sense.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAlXfvU1Zl8t"
      },
      "outputs": [],
      "source": [
        "# Because of some approaches I'll develop later I need\n",
        "# to build a direct mapping between fine and coarse-grained (we already have the opposite mapping)\n",
        "\n",
        "# d = json.load(open(\"data/mapping/cluster2fine_map.json\", \"r\"))\n",
        "# fine2coarse = {}\n",
        "# for k in d.keys():\n",
        "#     for fine_s in d[k]:\n",
        "#         fine2coarse[list(fine_s.keys())[0]] = k\n",
        "\n",
        "# json.dump(fine2coarse, open(\"data/map/fine2coarse.json\", \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how many <UNK> token we generate without any particular type of preprocessing!\n",
        "# data = WSD_DataModule(hparams)\n",
        "# data.setup()\n",
        "\n",
        "# tot_tokens = 0\n",
        "# tot_unk = 0\n",
        "# for batch in tqdm(data.train_dataloader()):\n",
        "#     for input in batch[\"inputs\"][\"input_ids\"]:\n",
        "#         for e in input:\n",
        "#             if e.item() == 0: # we reached <PAD> tokens\n",
        "#                 break\n",
        "#             tot_tokens+=1\n",
        "#             if e.item() == 100: # is the <UNK> token\n",
        "#                 tot_unk+=1\n",
        "# print(f\"We have a total of {tot_tokens} tokens\")\n",
        "# print(f\"with {tot_unk} <UNK> tokens!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Some quick analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:02<5:04:22,  2.59s/it]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [03:09<00:00, 37.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 97.09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  7%|▋         | 1/15 [00:00<00:07,  1.76it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            " 13%|█▎        | 2/15 [00:00<00:04,  2.85it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:11<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 97.14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:39,  1.54it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 93.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of 1 cluster candidates (so that the model cannot make wrong predictions)\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\") # 97.09% is high!\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1\n",
        "            \n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for cluster_candidates in b[\"cluster_candidates\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_candidates) == 1:\n",
        "            c+=1  \n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:50<00:00, 41.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  7%|▋         | 1/15 [00:00<00:09,  1.40it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:11<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:05,  2.34it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the average length of fine candidates\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"TRAIN - {round(tot_lenght/tot)}\") # 6.82\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"VAL - {round(tot_lenght/tot)}\")\n",
        "\n",
        "tot, tot_lenght = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for fine_candidates in b[\"fine_candidates\"]:\n",
        "        tot+=1\n",
        "        tot_lenght+=len(fine_candidates)\n",
        "print(f\"TEST - {round(tot_lenght/tot)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:00<57:37,  2.04it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 4/7064 [00:00<16:27,  7.15it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:59<00:00, 39.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:04<00:00, 33.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of CLUSTER gold labels of lenght 1\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for cluster_gold in b[\"cluster_gold\"]:\n",
        "        tot+=1\n",
        "        if len(cluster_gold) == 1:\n",
        "            c+=1\n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7064 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 1/7064 [00:00<1:13:13,  1.61it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  0%|          | 3/7064 [00:00<27:42,  4.25it/s]  Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 7064/7064 [02:54<00:00, 40.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN - 99.71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 15/15 [00:01<00:00, 12.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAL - 99.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/154 [00:00<?, ?it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "  1%|          | 1/154 [00:00<01:49,  1.39it/s]Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n",
            "100%|██████████| 154/154 [00:14<00:00, 10.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST - 87.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# to see the percentage of FINE gold labels of lenght 1\n",
        "hparams = asdict(Hparams())\n",
        "data = WSD_DataModule(hparams)\n",
        "data.setup()\n",
        "\n",
        "multi_label_list = []\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.train_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"TRAIN - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.val_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"VAL - {round((c/tot)*100, 2)}\")\n",
        "\n",
        "\n",
        "tot, c = 0, 0\n",
        "for b in tqdm(data.test_dataloader()):\n",
        "    for fine_gold in b[\"fine_gold\"]:\n",
        "        tot+=1\n",
        "        if len(fine_gold) == 1:\n",
        "            c+=1\n",
        "        else:\n",
        "            multi_label_list.append(fine_gold)\n",
        "print(f\"TEST - {round((c/tot)*100, 2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1259\n",
            "[[40065, 5311], [106465, 81781], [35163, 66176], [22597, 90145], [24054, 6462], [22531, 84295], [8830, 48103], [72599, 53724], [5028, 41282], [73948, 82496], [73351, 19959], [48063, 68594], [39521, 62072], [91731, 112520], [89660, 79547], [52737, 60262], [89660, 79547], [6033, 115027], [16751, 41934], [50745, 31796], [71472, 92433], [22434, 100346], [22913, 79262], [95627, 61787], [22434, 100346], [86188, 2367], [11384, 29211], [10341, 115318], [18383, 91728], [24645, 104754], [23650, 45962], [85529, 24008], [89660, 79547], [32272, 5308], [25146, 74697], [34284, 114942], [97064, 33390], [75319, 4215], [35520, 15441], [17701, 55524], [2169, 110751], [89660, 79547], [17558, 91944], [10697, 35452], [50021, 40818], [92483, 64732], [102625, 85897], [4726, 4109], [35773, 30789], [38634, 88327], [18873, 929], [87302, 86382], [5028, 53138], [35520, 15441], [2536, 45791], [78524, 72144], [71630, 68594], [66460, 81331], [116010, 13220], [28011, 86382], [16404, 18129], [2674, 34563], [15825, 34188], [6277, 31206], [91721, 41939], [90909, 62927], [97064, 33390], [55109, 33549], [111800, 19476], [10697, 35452], [99925, 6296], [4934, 108842], [60193, 111523], [35520, 15441], [45932, 40600], [70346, 82618], [54146, 39595], [62433, 8084], [108277, 69788], [8830, 22602], [81688, 10463], [64850, 4348], [85720, 44890], [87786, 60262], [109667, 70043], [117364, 17107], [91578, 91527], [69975, 35490], [59684, 79262], [34198, 93281], [54146, 39595], [10994, 44226], [73928, 79527], [82150, 78617], [81688, 53138], [114615, 64098], [92433, 63689], [60810, 115599], [111800, 117494], [105593, 56575], [64850, 4348], [73778, 117159], [49236, 62072], [35753, 2377], [629, 58502], [75319, 4215], [13379, 57924], [112329, 1048], [20856, 11555], [96213, 42222], [47955, 108561], [72955, 57297], [53138, 10463], [32450, 20566], [35241, 70343], [64850, 4348], [35520, 15441], [41282, 53138], [6277, 31206], [90664, 112329], [89660, 79547], [48233, 96737], [67782, 63311], [13629, 58502], [21162, 74677], [58214, 33828], [102814, 41178], [89660, 79547], [30248, 22290], [20280, 83259], [35773, 30789], [17558, 91944], [61277, 62072], [11371, 99589], [92483, 64732], [114083, 104623], [78787, 22545], [53517, 70747], [35520, 15441], [46096, 2348], [17558, 91944], [210, 48753], [22913, 79262], [109863, 75837], [17641, 108137], [110800, 91567], [81112, 76085], [111659, 57284], [32268, 74694], [60703, 11527], [107993, 39908], [96213, 42222], [18028, 99499], [82249, 91346], [106212, 55953], [59516, 71311], [78787, 22545], [87054, 20976], [61440, 25170], [13639, 97127], [39864, 90777], [50887, 50999], [28690, 96928], [72789, 59389], [103096, 104754], [35041, 92676], [73788, 43274], [65778, 27528], [85529, 24008], [16918, 18383], [2674, 34563], [107736, 13358], [62911, 64113], [46096, 2348], [59008, 13629], [29323, 104337], [15825, 34188], [104700, 60496], [61076, 856], [56611, 49594], [73788, 43274], [44647, 90145], [24067, 73037], [91804, 711], [19242, 80931], [64417, 83539], [56072, 28011], [95112, 115333], [102625, 85897], [78787, 22545], [18383, 64113], [77095, 42222], [32272, 5308], [38634, 7090], [73804, 111659], [103096, 104754], [111117, 68721], [110390, 67188], [89208, 108369], [39258, 92433], [59643, 95593], [3431, 81414], [88190, 103984], [41282, 53138], [112048, 54919], [5028, 53376], [35520, 15441], [76500, 105885], [91019, 51092], [35520, 15441], [10412, 31821], [107564, 91101], [78529, 60534], [57, 37279], [80304, 43229], [11371, 33828], [36039, 83327], [89660, 79547], [91578, 29701], [76180, 20143], [38539, 28714], [111227, 27751], [47955, 108561], [1048, 40600], [19881, 30017], [27475, 60970], [70820, 40428], [64732, 49050], [105116, 37395], [3042, 91731], [62911, 64113], [53138, 26630], [92483, 64732], [48373, 52329], [88368, 48833], [46442, 105124], [70850, 87054], [69055, 106640], [2674, 34563], [28479, 38042], [117364, 34049], [54146, 39595], [20001, 22677], [40065, 5311], [6277, 31206], [72904, 92704], [10944, 104945], [115896, 100503], [23787, 46691], [4488, 62911], [87054, 20976], [66781, 108084], [42746, 111689], [116254, 13109], [113788, 21046], [71770, 46705], [72450, 64739], [48373, 52329], [20320, 28642], [27532, 26407], [91944, 102519], [20147, 104395], [92433, 24008], [20147, 104395], [89660, 79547], [52105, 87617], [13265, 95364], [4836, 106817], [9349, 89090], [24978, 92483], [48706, 116164], [114352, 91441], [46096, 2348], [81688, 53138], [51416, 21333], [82193, 84647], [5243, 67914], [85499, 52141], [101805, 47669], [92483, 64732], [100810, 54910], [47955, 108561], [82130, 66813], [73001, 39258], [52301, 65676], [33539, 37216], [10075, 62072], [115896, 100503], [57828, 103213], [87528, 97158], [51429, 94222], [60193, 111523], [92483, 64732], [102625, 85897], [89660, 79547], [72450, 64739], [48200, 56949], [18834, 57654], [79742, 1034], [35520, 15441], [47055, 47763], [2674, 34563], [47955, 108561], [92483, 64732], [106619, 52364], [70820, 34327], [79048, 14144], [85529, 24008], [33961, 49743], [70219, 62927], [52301, 65676], [35520, 15441], [108122, 44593], [91236, 23393], [115896, 100503], [89660, 79547], [87302, 86382], [46340, 17114], [6277, 31206], [92433, 64257], [21305, 60741], [29323, 104337], [115803, 101033], [3042, 112520], [20001, 57094], [16897, 37269], [76072, 74381], [34550, 106267], [46095, 111266], [115896, 100503], [111497, 30814], [112329, 1048], [115896, 100503], [97387, 97066], [32118, 103530], [95180, 63675], [7423, 100499], [112329, 1048], [84541, 55129], [66891, 9276], [38634, 88327], [86378, 4343], [21081, 59051], [38634, 88327], [79048, 14144], [90090, 107593], [103922, 27574], [25909, 59998], [35773, 30789], [114656, 257], [104592, 90909], [102625, 85897], [87052, 107090], [16897, 37269], [40920, 92384], [89660, 79547], [103096, 104754], [16897, 37269], [67843, 103669], [35520, 15441], [109667, 70043], [99909, 11359], [35163, 66176], [35466, 107199], [89660, 79547], [89660, 79547], [22913, 79262], [5388, 8705, 32600], [71074, 25838], [110901, 2024], [70808, 109833], [30484, 60785], [53138, 26630], [99712, 72245], [83038, 96643], [89660, 79547], [35441, 116680], [40037, 61307], [45791, 93197], [34198, 57280], [92483, 64732], [73386, 32310], [79192, 60094], [52737, 60262], [74314, 65383], [7423, 100499], [69697, 40841], [23814, 48312], [6277, 31206], [52642, 26001], [21081, 59051], [37069, 6774], [108590, 94216], [72904, 92704], [90012, 28673], [84541, 55129], [97387, 97066], [71287, 63668], [64732, 49050], [73279, 73203], [24271, 94706], [46095, 111266], [107736, 13358], [85529, 24008], [2674, 34563], [93568, 29487], [32272, 5308], [78787, 22545], [69975, 5818], [59684, 79262], [73768, 28011], [78787, 22545], [107954, 43351], [72049, 29117], [108655, 97066], [11602, 257], [50021, 40818], [53665, 6882], [5118, 62346], [55225, 36801], [38634, 88327], [48373, 52329], [257, 47990], [103096, 104754], [52038, 21146], [40233, 97421], [51150, 60361], [91731, 112520], [16918, 18383], [87942, 23997], [24978, 92483], [76769, 60094], [42308, 60552], [115163, 82150], [103096, 104754], [92483, 64732], [13629, 71050], [48373, 52329], [35520, 15441], [113691, 51479], [108160, 92433, 404], [85216, 34198], [102155, 98032], [3074, 86382], [105247, 89940], [117141, 38462], [74011, 13639], [27360, 19064], [6887, 77579], [91731, 86382], [115896, 100503], [75343, 71842], [21402, 52598], [92483, 64732], [79347, 71444], [59516, 71311], [32763, 99340], [114656, 257], [63991, 44982], [8709, 91527], [114077, 49343], [108090, 41318], [78787, 22545], [2231, 104197], [84646, 62072], [85529, 92433], [89660, 79547], [81112, 76085], [20001, 57094], [26273, 36377], [87805, 97387], [52809, 32820], [105888, 116627], [84541, 55129], [98163, 50259], [28576, 11050], [104761, 117159], [103735, 72595], [92483, 64732], [89996, 34542], [95066, 117406], [2674, 34563], [87141, 57803], [62542, 23174], [4032, 105156], [35520, 15441], [42234, 116304], [94300, 21985], [97735, 12990], [60663, 113648], [90390, 3148], [23997, 51222], [90145, 45659], [81688, 53138], [16987, 33014], [92483, 64732], [52642, 26001], [4122, 61922], [15943, 92433], [94134, 59901], [45439, 105217], [115896, 100503], [19149, 53189], [92483, 64732], [91731, 112520], [20741, 45016], [10697, 35452], [53138, 26630], [59824, 75641], [20147, 104395], [73788, 43274], [9272, 14349], [111800, 117494], [73823, 112927], [73928, 79527], [72250, 63522], [4823, 34546], [59684, 79262], [53138, 2646], [88562, 48902], [35520, 15441], [39980, 45643], [22949, 49168], [51429, 94222], [1640, 62927], [3978, 103886], [95521, 1170], [3978, 103886], [102997, 46917], [78787, 22545], [36741, 110466], [19834, 39262], [47955, 108561], [3431, 81414], [85529, 15943], [85529, 24008], [68759, 24418], [64739, 108884], [50857, 82167], [39010, 37165], [92483, 64732], [52301, 65676], [72955, 57297], [73966, 24008], [115957, 74351], [24418, 70405], [54146, 39595], [73131, 106256], [54967, 13373], [94350, 58901], [73020, 53976], [53261, 106981], [78787, 22545], [87302, 86382], [48373, 52329], [92483, 64732], [20741, 45016], [103096, 104754], [6277, 31206], [73768, 28011], [37551, 35539], [70913, 59844], [85529, 24008], [96557, 77163], [24567, 20001], [92433, 24008], [2302, 70820], [72955, 57297], [4032, 105156], [33469, 20842], [103226, 82258], [20001, 22677], [92483, 64732], [20280, 83259], [73914, 95472], [27067, 60652], [92483, 64732], [100331, 42222], [12502, 2674], [35970, 24008], [63675, 11371], [87302, 86382], [51429, 94222], [75148, 96117], [47955, 108561], [70820, 9129], [8709, 86378], [73804, 111659], [92483, 64732], [110100, 94350], [110961, 33271], [22094, 93514], [54146, 39595], [85529, 24008], [18917, 61021], [78486, 5977], [20320, 28642], [53138, 26630], [61745, 78456], [85495, 70850], [11144, 25606], [92483, 64732], [89660, 79547], [81899, 40397], [102023, 53226], [91731, 112520], [31594, 38407], [71287, 63668], [111659, 56848], [47955, 108561], [68234, 106757, 68267], [116538, 31938], [12502, 2674], [59516, 71311], [2302, 44723], [81112, 76085], [94798, 18673], [7423, 100499], [67471, 25625], [20280, 83259], [92483, 64732], [29220, 52022], [111800, 79984], [111800, 117494], [50811, 52770], [2646, 10463], [6277, 31206], [60810, 93334], [39521, 62072], [17558, 91944], [15825, 34188], [91415, 86923], [32598, 94882], [8830, 48103], [52737, 60262], [32118, 103530], [12502, 2674], [105593, 56575], [114218, 56072], [6839, 16699], [6277, 31206], [5028, 53376], [61503, 85799], [87786, 60262], [65134, 78973], [58214, 33828], [5785, 694], [50857, 82167], [53590, 38462], [26462, 75931], [33887, 1663], [111800, 19476], [81452, 73001], [77284, 110512], [12502, 2674], [85529, 24008], [116775, 47871], [47663, 43321], [29557, 47969], [92056, 33828], [53100, 40005], [76946, 17996], [45176, 65319], [45176, 65319], [12719, 41723], [35668, 102843], [45176, 65319], [53100, 40005], [53100, 40005], [105771, 103956], [53100, 40005], [45176, 65319], [98474, 72910], [77842, 85166, 51435], [92647, 79645], [13295, 116301], [58674, 19742], [105593, 56575, 37351], [105593, 56575, 37351], [72928, 59389], [105593, 56575, 37351], [105593, 56575, 37351], [42340, 22058], [21590, 65944], [84749, 2502], [35962, 48752], [37275, 11161, 27988], [93716, 109191], [42340, 22058], [93716, 109191], [36336, 67569], [96714, 67003], [37275, 11161, 27988], [42340, 22058], [38251, 23489, 2314], [56848, 88860], [63623, 83235], [8904, 93395], [56575, 37351], [59269, 116826], [12719, 41723], [105815, 65136], [87006, 52642], [74584, 106979, 42764, 76870], [15016, 96654, 78467], [105815, 65136], [67569, 67974], [36336, 67569, 106849], [20688, 68115, 42916], [105815, 65136], [59516, 71311, 32118], [105593, 56575, 37351], [87006, 52642], [92746, 74263], [44647, 90145], [12719, 41723, 49159], [74220, 20940], [110602, 65830], [110602, 65830], [93417, 85216], [87006, 52642], [62695, 63809], [21816, 2884], [18933, 75469], [87006, 52642], [51606, 100603], [56848, 111659], [103530, 32118], [70223, 99270], [51606, 100603], [56848, 111659], [51606, 100603], [38251, 23489, 2314], [56848, 111659], [8904, 93395], [21590, 65944], [51606, 100603], [70223, 99270], [34563, 2674], [51606, 100603], [257, 114656, 47886], [20688, 68115], [92985, 52886, 83560], [76946, 17996], [93067, 48944, 1269], [76946, 17996], [44647, 90145], [35520, 15441], [18383, 91728], [34563, 2674], [56848, 111659], [93417, 85216], [93417, 85216], [34563, 2674], [34563, 2674], [26434, 102173, 67965], [56848, 111659], [26434, 102173, 67965], [58216, 99383], [26434, 67965], [87006, 52642], [56848, 111659], [8904, 93395], [44647, 90145], [34563, 112940, 91738], [34563, 2674], [37269, 16897], [92647, 79645], [92647, 79645], [92647, 79645], [92647, 79645], [112413, 11328], [11497, 44260], [25625, 97723], [13639, 74011], [34059, 50394], [36203, 18673], [34059, 50394], [36203, 18673], [102780, 8255], [65110, 62542, 18673], [78633, 85594, 25770], [76789, 11371], [44249, 46520], [31601, 59898, 59705], [34563, 2674], [34059, 50394], [36203, 18673], [70346, 30951, 19964], [112382, 109727], [52809, 65383], [23997, 87942, 51222], [100068, 8102, 106300], [100068, 8102, 106300], [5388, 8705], [17650, 24175], [17075, 52821], [21437, 91944, 17558], [38481, 16570], [35959, 91731], [34327, 43873], [26399, 17064, 93006], [34870, 28096, 84560], [87006, 52642], [87006, 52642], [40600, 112329], [87006, 52642], [90733, 85230], [40600, 112329], [92433, 64257], [87006, 52642], [37069, 2732], [41895, 96259], [32546, 19481], [41895, 96259], [21437, 17558], [81994, 101214], [50654, 53295], [22127, 78590], [7090, 8709], [44647, 90145], [15336, 13309], [40600, 112329], [30797, 99710], [10964, 72764], [9007, 16136], [101688, 79161], [12271, 34546], [5311, 40065], [20001, 22677], [36203, 18673], [83994, 117122], [35668, 102843], [45791, 84052], [44647, 90145], [78991, 77217, 107954], [85269, 55108], [76946, 17996], [26434, 102173, 67965], [101688, 79161], [2536, 45791, 84052, 41301], [64739, 28121], [8084, 78582], [26434, 102173, 67965], [44647, 90145], [34563, 2674], [2536, 45791, 84052, 41301], [5311, 40065], [83585, 84211], [64739, 28121], [110800, 91567], [82025, 629], [81452, 108362], [103057, 19747], [89469, 116136], [103057, 19747], [106919, 96022], [112413, 11328], [67471, 25625, 97723], [92433, 64257], [74843, 39522], [74843, 39522], [74843, 39522], [26946, 76815], [35753, 2377], [52809, 65383], [78617, 52809], [23997, 87942, 51222], [2279, 26363], [115734, 62777], [15939, 39369], [106979, 91567], [1794, 59958], [54094, 28111], [18834, 57654], [52141, 25541], [111542, 2377], [41378, 77818], [28479, 38042], [114932, 56815], [43229, 80304, 43005], [114932, 56815], [86410, 12462, 78114], [98555, 72054], [99965, 43166, 78401, 53843], [79703, 93461], [64750, 42697, 111660], [40377, 56649], [13639, 56501], [38937, 28045], [66806, 264], [5388, 8705], [31784, 57715], [114932, 56815], [65580, 18695], [4171, 58502, 13629], [9300, 83726], [37279, 20786, 57], [45643, 62399], [95180, 112731], [8998, 72139], [42891, 37542], [64969, 62722], [64969, 62722], [67189, 76610], [68594, 76443], [34090, 43274], [69804, 62545], [18383, 16918], [79563, 2577], [5311, 40065], [56848, 111659], [95482, 60703, 69254], [34563, 2674], [52737, 3504], [86410, 12462, 78114], [52737, 87949], [68594, 71630, 76443], [97295, 1607], [93096, 96781], [56848, 111659], [37216, 88368], [34563, 2674], [96213, 100331], [56079, 24576], [41248, 5792, 29099], [30327, 100544], [25191, 40400], [86382, 37409], [21773, 11602], [52737, 87949], [68594, 48063, 76443, 85145], [68594, 48063, 76443, 85145], [33003, 4485], [97214, 21816], [45443, 40600, 43542, 112329], [69975, 35490], [21420, 97400], [68594, 48063, 76443, 85145], [105288, 102519], [34327, 43873], [48110, 22232], [45443, 22087, 43542, 112329], [105288, 102519], [98218, 81881], [110509, 13174], [68594, 48063, 76443, 85145], [68594, 48063, 76443, 85145], [69975, 35490], [69975, 35490], [111639, 109862], [68594, 48063, 76443, 85145], [19565, 11983], [78617, 52809], [69203, 110961], [78991, 77217, 107954], [53760, 63225], [91136, 111950], [40926, 76441], [100015, 5295], [96259, 101479], [45443, 40600, 43542, 112329], [60663, 113648], [30797, 99710], [114989, 3198], [32493, 22822], [15559, 62158], [60663, 113648], [110509, 13174], [87006, 52642], [87006, 52642], [45443, 40600, 43542, 112329], [107135, 34816], [23174, 62542], [101342, 16454], [45443, 40600, 46931], [64739, 8181], [45443, 40600, 43542, 112329], [78617, 52809], [101342, 16454], [31206, 6277], [61160, 8346], [101342, 16454], [116254, 13109], [64969, 62722], [85216, 66999], [64969, 62722], [85216, 66999], [31601, 59898, 59705], [90230, 49594], [10964, 72764], [110509, 13174], [110509, 13174], [60663, 113648], [101342, 16454], [30797, 99710], [107135, 34816], [23174, 62542], [45443, 40600, 43542, 112329], [90480, 71481], [62695, 63809], [45443, 40600, 43542, 112329], [45443, 40600, 43542, 112329], [11602, 15717], [11602, 15717], [45443, 40600, 43542, 112329], [45443, 40600, 43542, 112329], [53760, 63225], [97214, 21816, 2884], [99444, 20061], [114989, 92548], [60663, 113648], [17075, 52821], [45443, 40600, 43542, 112329], [52737, 87949], [64969, 62722], [90642, 61976], [45443, 40600, 43542, 112329], [30797, 99710], [97214, 21816, 2884], [69795, 29118], [68594, 48063, 76443, 85145], [52737, 87949], [68594, 52294, 48063, 85145], [11602, 15717], [112382, 109727], [78991, 77217, 107954], [74584, 106979, 42764, 76870], [23997, 87942], [35576, 92004], [45659, 90145], [4381, 15745], [12802, 100180], [42482, 58202], [93417, 85216], [92409, 34192], [110499, 111224], [45659, 90145], [74584, 106979, 42764, 76870], [5388, 8705], [74584, 106979, 42764, 76870], [68305, 114442], [97066, 91703], [45659, 90145], [7952, 110499], [14023, 63877], [105666, 9732], [78991, 77217, 107954], [96964, 14218], [34563, 2674], [34563, 2674], [76731, 94800], [4381, 15745], [56414, 14145], [78991, 77217, 107954], [60663, 113648], [60663, 113648], [78991, 77217, 107954], [9466, 11985], [69975, 35490, 117035], [24895, 54301], [5407, 112497, 2274], [76946, 17996], [56848, 57284], [257, 114656], [39779, 40544], [69975, 35490], [60074, 36893], [13295, 116301], [44093, 42614], [55161, 101662], [4381, 15745], [4381, 15745], [92746, 74263], [74584, 106979, 42764, 76870], [5388, 8705], [110499, 76361], [13295, 116301], [1345, 77436], [74584, 106979, 42764, 76870], [68594, 76443], [57828, 103213], [17075, 52821], [1345, 77436], [13295, 116301], [87006, 52642], [56052, 107108], [97295, 1607, 8695], [56052, 107108], [940, 79194], [2131, 35517], [74584, 106979, 42764], [78991, 77217, 107954], [96134, 97708], [96718, 90892], [65778, 82993], [87006, 52642], [13295, 116301], [97387, 97066, 63605], [114146, 77586], [3042, 86382], [94216, 12167], [59643, 12769], [92433, 57643], [99795, 50980, 33828], [21437, 91944, 17558], [28404, 46103], [99795, 50980, 33828], [52737, 87949], [69545, 108096], [2062, 34563, 112940], [37279, 20786, 57], [99795, 50980, 33828], [70850, 90754, 86382], [9009, 60545, 116993], [52602, 33271], [88542, 67426, 62553], [83288, 39908], [21437, 91944, 17558], [12990, 97735], [99795, 33828], [92412, 107710], [64561, 92508], [79262, 59684], [69975, 35490], [69975, 35490], [59643, 12769], [99900, 101208], [43229, 80304, 43005], [91804, 93736], [105593, 37351], [105288, 102519], [110726, 2049], [105288, 102519], [110726, 2049], [109547, 20796], [56538, 46826, 68795], [52602, 33271], [83288, 39908], [30562, 108471], [87006, 52642], [87006, 52642], [66245, 29117], [2062, 34563, 112940, 24944], [2062, 34563, 112940, 24944], [76582, 30246], [64864, 20435], [18383, 91728], [75895, 44718], [52737, 87949], [92433, 71472], [34090, 43274], [4843, 60262], [99795, 33828], [21437, 91944, 17558], [99795, 33828], [62967, 43011], [87006, 52642], [8064, 92409], [4823, 34546], [5388, 8705], [88576, 88001], [63720, 21480], [56848, 88860], [92433, 57643], [48704, 68078, 9961, 2541], [2536, 45791, 84052, 41301], [56848, 88860], [88576, 88001], [92433, 57643], [2536, 45791, 84052, 41301], [23997, 87942], [39501, 53931], [5388, 8705], [34563, 2674], [39501, 53931], [94622, 17558], [2536, 45791, 84052, 41301], [76789, 92433, 64257, 57643], [87790, 37396], [96213, 100331], [76731, 94800], [2536, 45791, 84052, 41301], [91731, 114116], [74584, 106979, 42764], [92433, 57643], [2536, 45791, 84052, 41301], [60663, 113648], [88576, 88001], [45443, 40600, 112329], [83994, 117122], [25637, 100745], [83994, 117122], [16553, 17937], [88576, 88001], [76789, 92433, 64257, 57643], [2536, 45791, 84052, 41301], [16553, 17937], [52809, 111208, 42742], [83994, 117122], [2536, 45791, 84052, 41301], [59516, 71311, 103530, 32118], [83994, 117122], [79984, 111800, 117494], [83994, 117122], [60663, 113648], [19684, 75857], [53760, 63225], [59898, 59705], [4823, 34546], [56848, 88860], [90230, 49594], [96564, 78434, 92673], [8926, 99971], [107593, 90090], [32546, 19481], [43475, 32196], [112759, 95961, 53521, 2355], [54800, 27726], [104318, 19805, 109412], [112715, 6295], [91674, 14742], [49860, 47816], [112759, 95961, 53521, 2355], [58625, 48816], [37279, 20786, 57], [87006, 52642], [62174, 65110, 18673], [105668, 101402, 43985, 25106], [29882, 16883], [112759, 95961, 53521, 2355], [112759, 95961, 53521, 2355], [36203, 65110, 18673], [56949, 68936], [100426, 100095], [16107, 86680], [10268, 78459], [64113, 4488], [28479, 38042], [37279, 20786, 57], [88263, 32493], [5791, 75148], [47150, 3612], [91731, 114116], [91848, 8064, 92409], [34090, 43274], [40474, 84497], [37279, 20786, 57], [40474, 84497], [47144, 81881], [86382, 3074], [36033, 59297], [52873, 117353], [68594, 76443], [112759, 95961, 53521, 2355], [80291, 15952], [40474, 84497], [107643, 14771, 2896], [5959, 2667]]\n",
            "Counter({2: 1122, 3: 89, 4: 48})\n",
            "2703\n",
            "['print.v.01', 'publish.v.02', 'casual.s.04', 'casual.s.05', 'construct.v.01', 'do.v.08', 'arrangement.n.03', 'administration.n.02', 'drag.v.05', 'chase.v.01', 'admirable.s.02', 'admirable.s.01', 'depart.v.03', 'take_off.v.03', 'figure.n.08', 'name.n.04', 'be.v.02', 'embody.v.02', 'earth.n.01', 'earth.n.04', 'belated.s.01', 'late.a.06', 'fresh.s.04', 'new.a.01', 'full.s.06', 'good.a.01', 'understand.v.02', 'see.v.05', 'prime_minister.n.01', 'chancellor.n.02', 'allege.v.01', 'state.v.01', 'prime_minister.n.01', 'chancellor.n.02', 'love.n.04', 'love.n.01', 'solid.s.05', 'solid.a.02', 'area.n.01', 'area.n.03', 'own.v.01', 'have.v.01', 'nod.v.02', 'nod.v.01', 'serviceman.n.01', 'man.n.01', 'direct.v.04', 'direct.v.01', 'nod.v.02', 'nod.v.01', 'glorify.v.02', 'laud.v.01', 'phrase.n.02', 'phrase.n.01', 'experience.v.01', 'know.v.05', 'topic.n.02', 'subject.n.01', 'case.n.06', 'case.n.01', 'honest.s.02', 'honest.a.01', 'keep.v.01', 'hold.v.02', 'prime_minister.n.01', 'chancellor.n.02', 'connection.n.03', 'connection.n.02', 'call.v.03', 'call.v.05', 'wave.n.02', 'wave.n.01', 'wit.n.01', 'humor.n.02', 'mirror_image.n.01', 'contemplation.n.02', 'year.n.01', 'year.n.02', 'vision.n.01', 'vision.n.03', 'decision.n.01', 'decision.n.02', 'prime_minister.n.01', 'chancellor.n.02', 'do.v.03', 'make.v.01', 'listen.v.02', 'heed.v.01', 'glory.n.02', 'glory.n.01', 'spirit.n.01', 'spirit.n.02', 'shoot.v.01', 'shoot.v.02', 'widely.r.01', 'wide.r.04', 'day.n.01', 'day.n.05', 'cover.v.05', 'consider.v.03', 'board.n.02', 'board.n.03', 'visit.v.01', 'see.v.01', 'be.v.02', 'be.v.01', 'year.n.01', 'year.n.02', 'national.a.02', 'national.a.03', 'comment.v.01', 'comment.v.02', 'raw.s.12', 'new.a.01', 'retire.v.02', 'withdraw.v.01', 'discontinue.v.01', 'drop_out.v.01', 'meet.v.01', 'see.v.01', 'attention.n.01', 'attention.n.03', 'problem.n.01', 'trouble.n.01', 'flicker.n.01', 'sparkle.n.01', 'lead.v.03', 'contribute.v.03', 'drive.v.05', 'force.v.06', 'occupation.n.01', 'job.n.02', 'wit.n.01', 'humor.n.02', 'work.n.02', 'work.n.01', 'control.n.01', 'restraint.n.02', 'listen.v.02', 'heed.v.01', 'anguish.n.02', 'anguish.n.01', 'penalty.n.02', 'punishment.n.01', 'home.n.01', 'dwelling.n.01', 'year.n.01', 'year.n.02', 'decide.v.01', 'determine.v.01', 'kill.v.01', 'stamp_out.v.01', 'troop.n.02', 'troop.n.01', 'background.n.01', 'background.n.03', 'restrain.v.01', 'restrict.v.03', 'depart.v.03', 'start.v.08', 'exist.v.01', 'be.v.05', 'essential.s.01', 'essential.a.02', 'well.r.02', 'well.r.01', 'tell.v.03', 'state.v.01', 'shot.n.02', 'shooting.n.01', 'light.n.06', 'luminosity.n.01', 'worry.v.02', 'care.v.01', 'know.v.01', 'know.v.03', 'man.n.03', 'man.n.01', 'item.n.01', 'detail.n.01', 'troop.n.02', 'troop.n.01', 'competitive.a.01', 'competitive.s.03', 'box.n.03', 'box.n.01', 'indicate.v.02', 'prove.v.02', 'exist.v.01', 'be.v.01', 'run.v.01', 'scat.v.01', 'have.v.01', 'retain.v.03', 'tough.a.01', 'ruffianly.s.01', 'control.n.01', 'control.n.02', 'end.n.05', 'end.n.03', 'essential.s.01', 'essential.a.02', 'darkness.n.02', 'iniquity.n.01', 'good.a.03', 'good.a.01', 'special.s.04', 'particular.s.01', 'give.v.05', 'give.v.01', 'mirror_image.n.01', 'contemplation.n.02', 'plan.v.02', 'plan.v.03', 'detect.v.01', 'find.v.01', 'like.v.02', 'like.v.05', 'function.n.03', 'character.n.04', 'tone.n.02', 'timbre.n.01', 'evolve.v.01', 'develop.v.01', 'be.v.01', 'be.v.05', 'load.n.02', 'cargo.n.01', 'inaugural.s.02', 'first.a.01', 'essential.s.01', 'essential.a.02', 'year.n.01', 'year.n.02', 'embody.v.02', 'be.v.01', 'lead.v.03', 'contribute.v.03', 'find.v.03', 'detect.v.01', 'prime_minister.n.01', 'chancellor.n.02', 'color.n.02', 'semblance.n.01', 'answer.v.01', 'answer.v.05', 'yield.v.01', 'give.v.01', 'normal.a.01', 'normal.a.02', 'feel.v.03', 'feel.v.01', 'fun.n.01', 'fun.n.02', 'prime_minister.n.01', 'chancellor.n.02', 'sweep.v.02', 'sail.v.01', 'gun.n.01', 'artillery.n.01', 'day.n.01', 'day.n.05', 'do.v.03', 'make.v.01', 'adept.s.01', 'good.a.01', 'experience.v.03', 'pick_up.v.09', 'spirit.n.01', 'spirit.n.02', 'fool.n.01', 'chump.n.01', 'clear.s.02', 'clear.a.01', 'estimate.n.01', 'appraisal.n.02', 'year.n.01', 'year.n.02', 'village.n.02', 'village.n.01', 'do.v.03', 'make.v.01', 'assembly.n.06', 'gathering.n.01', 'serviceman.n.01', 'man.n.01', 'field.n.01', 'plain.n.01', 'dramatic.s.02', 'dramatic.a.01', 'causal_agent.n.01', 'cause.n.01', 'agency.n.02', 'agency.n.01', 'country.n.02', 'nation.n.02', 'across-the-board.s.01', 'wide.a.01', 'care.n.01', 'care.n.04', 'wish.v.02', 'wish.v.01', 'function.n.03', 'character.n.04', 'back.n.03', 'rear.n.02', 'advanced.s.03', 'modern.a.01', 'lethargy.n.01', 'inanition.n.01', 'change.n.03', 'change.n.01', 'clear.s.02', 'clear.a.01', 'hear.v.04', 'hear.v.01', 'world.n.03', 'universe.n.01', 'want.v.02', 'desire.v.01', 'raise.v.02', 'lift.v.03', 'classify.v.01', 'distinguish.v.01', 'justice.n.02', 'justice.n.01', 'return.n.03', 'return.n.02', 'event.n.02', 'case.n.01', 'slavery.n.02', 'bondage.n.02', 'come.v.03', 'arrive.v.01', 'powerful.a.01', 'knock-down.s.01', 'keep.v.01', 'hold.v.02', 'issue.n.01', 'topic.n.02', 'problem.n.01', 'trouble.n.01', 'dream.n.02', 'ambition.n.01', 'thing.n.01', 'matter.n.01', 'village.n.02', 'village.n.01', 'grant.v.05', 'yield.v.01', 'military_action.n.01', 'action.n.01', 'flicker.n.01', 'sparkle.n.01', 'interest.n.03', 'interest.n.01', 'dismiss.v.01', 'dismiss.v.02', 'research.v.02', 'search.v.02', 'come.v.03', 'arrive.v.01', 'organization.n.01', 'administration.n.02', 'fog.n.01', 'fog.n.02', 'real.a.02', 'real.a.01', 'wedding.n.01', 'marriage.n.03', 'aid.n.01', 'aid.n.03', 'see.v.15', 'meet.v.01', 'false.a.01', 'false.s.02', 'shoot.v.01', 'shoot.v.02', 'clear.s.02', 'clear.a.01', 'topic.n.02', 'matter.n.01', 'fictional_character.n.01', 'character.n.04', 'connection.n.03', 'connection.n.02', 'cover.v.05', 'deal.v.03', 'domain.n.02', 'country.n.02', 'event.n.02', 'case.n.01', 'confirm.v.01', 'corroborate.v.03', 'prosperity.n.01', 'prosperity.n.02', 'coconut.n.02', 'coconut.n.01', 'accept.v.02', 'have.v.01', 'sit.v.01', 'sit.v.02', 'consonant.n.01', 'consonant.n.02', 'verse.n.02', 'poetry.n.01', 'embody.v.02', 'be.v.01', 'model.n.07', 'example.n.01', 'be.v.02', 'be.v.08', 'year.n.01', 'year.n.02', 'tell.v.05', 'assure.v.02', 'mission.n.03', 'mission.n.02', 'year.n.01', 'year.n.02', 'suggest.v.03', 'hint.v.01', 'pray.v.01', 'beg.v.01', 'refuse.v.02', 'refuse.v.01', 'watch.v.02', 'watch.v.01', 'enjoy.v.01', 'delight.v.02', 'experience.v.03', 'feel.v.01', 'pull.v.01', 'draw.v.16', 'prime_minister.n.01', 'chancellor.n.02', 'worry.v.02', 'worry.v.01', 'tooth.n.01', 'tooth.n.03', 'probe.n.01', 'investigation.n.02', 'agreement.n.04', 'agreement.n.01', 'tone.n.02', 'timbre.n.01', 'find.v.01', 'determine.v.01', 'disperse.v.02', 'separate.v.08', 'significantly.r.01', 'significantly.r.02', 'travel.v.01', 'move.v.04', 'spirit.n.02', 'spirit.n.03', 'aspect.n.02', 'aspect.n.01', 'visualize.v.01', 'understand.v.02', 'thing.n.01', 'matter.n.01', 'be.v.01', 'constitute.v.01', 'spirit.n.01', 'spirit.n.02', 'defense.n.02', 'defense.n.01', 'establish.v.01', 'lay_down.v.01', 'lurch.v.03', 'stagger.v.01', 'learn.v.02', 'hear.v.04', 'business.n.01', 'commercial_enterprise.n.02', 'problem.n.01', 'trouble.n.01', 'door.n.01', 'doorway.n.01', 'light.n.06', 'light.n.07', 'troop.n.02', 'troop.n.01', 'survey.n.01', 'report.n.01', 'print.v.01', 'publish.v.02', 'lead.v.03', 'contribute.v.03', 'abroad.r.01', 'afield.r.01', 'branch.n.03', 'leg.n.01', 'rule.n.04', 'convention.n.02', 'adequate.s.02', 'adequate.a.01', 'thing.n.05', 'thing.n.01', 'hear.v.04', 'hear.v.01', 'mile.n.01', 'nautical_mile.n.02', 'specify.v.03', 'specify.v.02', 'large.a.01', 'big.s.05', 'effort.n.02', 'attempt.n.01', 'emergence.n.01', 'growth.n.02', 'study.v.03', 'analyze.v.01', 'defense.n.02', 'defense.n.01', 'theology.n.01', 'theology.n.02', 'check.v.01', 'investigate.v.01', 'make.v.01', 'cause.v.01', 'lie.v.02', 'lie.v.01', 'have.v.01', 'hold.v.02', 'lie.v.02', 'lie.v.01', 'prime_minister.n.01', 'chancellor.n.02', 'celebration.n.02', 'celebration.n.01', 'dim.s.02', 'faint.s.03', 'publish.v.03', 'write.v.01', 'buzz.v.01', 'buzz.v.02', 'spirit.n.04', 'spirit.n.01', 'more.a.01', 'more.a.02', 'corrupt.v.04', 'botch.v.01', 'village.n.02', 'village.n.01', 'exist.v.01', 'be.v.01', 'meet.v.11', 'receive.v.01', 'note.v.03', 'note.v.01', 'record.n.05', 'record.n.04', 'citizenry.n.01', 'people.n.01', 'excel.v.01', 'leap_out.v.01', 'spirit.n.01', 'spirit.n.02', 'authentic.s.01', 'reliable.a.01', 'tone.n.02', 'timbre.n.01', 'court.n.03', 'court.n.01', 'take.v.08', 'accept.v.02', 'name.v.01', 'call.v.02', 'install.v.03', 'establish.v.02', 'good.s.06', 'good.a.01', 'rule.n.04', 'convention.n.02', 'money.n.01', 'money.n.02', 'stop.n.02', 'arrest.n.02', 'act.v.01', 'act.v.04', 'home.n.01', 'dwelling.n.01', 'spirit.n.01', 'spirit.n.02', 'shoot.v.01', 'shoot.v.02', 'prime_minister.n.01', 'chancellor.n.02', 'study.v.03', 'analyze.v.01', 'pile.n.01', 'batch.n.02', 'especial.s.01', 'particular.s.02', 'shout.v.02', 'holler.v.01', 'year.n.01', 'year.n.02', 'manzanita.n.01', 'madrona.n.01', 'problem.n.01', 'trouble.n.01', 'tone.n.02', 'timbre.n.01', 'spirit.n.01', 'spirit.n.02', 'undefined.a.01', 'obscure.s.01', 'travel.v.01', 'function.v.01', 'deal.n.01', 'trade.n.01', 'keep.v.01', 'hold.v.02', 'add.v.04', 'add.v.01', 'job.n.05', 'job.n.02', 'name.v.01', 'call.v.02', 'year.n.01', 'year.n.02', 'change.v.05', 'switch.v.03', 'carry.v.02', 'carry.v.05', 'rule.n.04', 'convention.n.02', 'prime_minister.n.01', 'chancellor.n.02', 'visit.v.01', 'see.v.01', 'develop.v.03', 'grow.v.08', 'lead.v.03', 'contribute.v.03', 'have.v.01', 'have.v.02', 'communication.n.02', 'communication.n.01', 'military_action.n.01', 'action.n.01', 'seed.n.02', 'seed.n.01', 'visualize.v.01', 'see.v.05', 'survey.n.01', 'study.n.02', 'new_york.n.01', 'new_york.n.02', 'railroad_track.n.01', 'railway.n.01', 'association.n.04', 'association.n.03', 'politics.n.03', 'politics.n.01', 'rule.n.04', 'convention.n.02', 'banana.n.01', 'banana.n.02', 'detect.v.01', 'find.v.01', 'rule.n.04', 'convention.n.02', 'think.v.01', 'think.v.02', 'change.n.02', 'change.n.04', 'receive.v.02', 'get.v.01', 'resource.n.02', 'resource.n.01', 'detect.v.01', 'find.v.01', 'sport.n.02', 'sport.n.01', 'wealth.n.03', 'wealth.n.01', 'cover.v.05', 'consider.v.03', 'treat.v.01', 'treat.v.03', 'range.n.02', 'scope.n.01', 'cover.v.05', 'consider.v.03', 'deal.n.01', 'trade.n.01', 'graphic.s.05', 'vivid.s.02', 'impossible.s.02', 'impossible.a.01', 'hard.a.03', 'hard.a.02', 'day.n.01', 'day.n.05', 'development.n.02', 'development.n.01', 'job.n.03', 'occupation.n.01', 'shoot.v.01', 'shoot.v.02', 'misfortune.n.02', 'misfortune.n.01', 'new_york.n.01', 'new_york.n.02', 'case.n.05', 'case.n.10', 'prime_minister.n.01', 'chancellor.n.02', 'event.n.02', 'case.n.01', 'new_york.n.01', 'new_york.n.02', 'astute.s.01', 'acute.s.03', 'year.n.01', 'year.n.02', 'shot.n.02', 'shooting.n.01', 'lyric.a.02', 'lyric.s.01', 'construct.v.01', 'do.v.08', 'sing.v.01', 'sing.v.02', 'prime_minister.n.01', 'chancellor.n.02', 'prime_minister.n.01', 'chancellor.n.02', 'serviceman.n.01', 'man.n.01', 'simple.a.01', 'elementary.s.01', 'bare.s.06', 'hold_up.v.04', 'delay.v.01', 'military_unit.n.01', 'force.n.04', 'reconcile.v.03', 'decide.v.02', 'floor.n.05', 'floor.n.04', 'be.v.01', 'constitute.v.01', 'english.n.03', 'english.n.01', 'carry_through.v.01', 'meet.v.04', 'prime_minister.n.01', 'chancellor.n.02', 'peace.n.02', 'peace.n.01', 'fall.n.05', 'spill.n.04', 'national.a.03', 'national.s.04', 'item.n.01', 'item.n.03', 'spirit.n.01', 'spirit.n.02', 'choose.v.01', 'take.v.04', 'expect.v.03', 'wait.v.01', 'allege.v.01', 'state.v.01', 'express.v.01', 'show.v.04', 'resource.n.02', 'resource.n.01', 'incite.v.02', 'motivate.v.01', 'semiliterate.s.02', 'semiliterate.s.01', 'lead.v.03', 'contribute.v.03', 'manner.n.01', 'manner.n.02', 'range.n.02', 'scope.n.01', 'last.a.02', 'concluding.s.01', 'stimulate.v.03', 'induce.v.02', 'abroad.r.01', 'afield.r.01', 'firm.s.02', 'solid.s.04', 'sport.n.02', 'sport.n.01', 'think.v.01', 'think.v.02', 'product.n.02', 'merchandise.n.01', 'spirit.n.02', 'spirit.n.03', 'ceremony.n.02', 'ceremony.n.01', 'eye.n.02', 'eye.n.01', 'politics.n.03', 'politics.n.01', 'dream.n.02', 'ambition.n.01', 'keep.v.01', 'hold.v.02', 'problem.n.01', 'trouble.n.01', 'dense.s.04', 'dull.a.01', 'connection.n.03', 'connection.n.02', 'clear.s.02', 'clear.a.01', 'know.v.01', 'know.v.02', 'man.n.03', 'man.n.01', 'meet.v.08', 'meet.v.01', 'clear.s.02', 'clear.a.01', 'significant.a.01', 'significant.s.02', 'face.n.05', 'face.n.01', 'calculate.v.02', 'think.v.02', 'growth.n.01', 'development.n.01', 'glory.n.02', 'glory.n.01', 'newspaper.n.01', 'newspaper.n.02', 'crash.n.02', 'clang.n.01', 'expose.v.03', 'unwrap.v.02', 'cover.v.05', 'consider.v.03', 'defense.n.02', 'defense.n.01', 'development.n.01', 'exploitation.n.01', 'event.n.02', 'case.n.01', 'gay.s.02', 'gay.s.03', 'gold.n.01', 'gold.n.03', 'map.v.01', 'map.v.02', 'understand.v.02', 'see.v.05', 'issue.n.01', 'topic.n.02', 'effective.s.02', 'effective.a.01', 'spirit.n.04', 'spirit.n.01', 'wait.v.02', 'wait.v.01', 'fight.n.02', 'combat.n.01', 'indicate.v.03', 'indicate.v.02', 'event.n.02', 'case.n.01', 'spirit.n.01', 'spirit.n.02', 'yield.v.01', 'give.v.18', 'defense.n.02', 'defense.n.01', 'year.n.01', 'year.n.02', 'academic.s.02', 'academic.s.03', 'take.v.35', 'have.v.01', 'have.v.07', 'degree.n.02', 'item.n.01', 'symphony_orchestra.n.01', 'symphony.n.01', 'watch.v.03', 'see.v.01', 'information.n.03', 'information.n.01', 'direction.n.02', 'direction.n.01', 'need.v.03', 'want.v.02', 'silver.s.02', 'silver.s.01', 'deny.v.04', 'deny.v.03', 'understand.v.02', 'see.v.01', 'rule.n.04', 'convention.n.02', 'break.v.03', 'break.v.05', 'flag.n.01', 'masthead.n.01', 'spirit.n.01', 'spirit.n.02', 'meat_house.n.02', 'smokehouse.n.01', 'change.n.03', 'change.n.01', 'precisely.r.01', 'precisely.r.03', 'development.n.02', 'development.n.01', 'busy.v.01', 'absorb.v.09', 'manage.v.02', 'care.v.01', 'clear.v.10', 'light.v.01', 'influence.n.02', 'influence.n.01', 'clear.s.02', 'clear.a.01', 'public.s.02', 'public.a.01', 'estimable.s.02', 'good.a.01', 'keep.v.01', 'have.v.01', 'prime_minister.n.01', 'chancellor.n.02', 'agency.n.02', 'agency.n.01', 'survey.n.01', 'study.n.02', 'patriotism.n.01', 'nationalism.n.02', 'think.v.03', 'think.v.01', 'testify.v.02', 'show.v.01', 'paint.v.02', 'paint.v.01', 'sport.n.02', 'sport.n.01', 'service.n.04', 'service.n.02', 'opinion.n.04', 'opinion.n.05', 'dark.n.01', 'iniquity.n.01', 'enclose.v.02', 'control.v.02', 'spirit.n.01', 'spirit.n.02', 'capital.n.02', 'capital.n.01', 'shelter.n.01', 'shelter.n.02', 'problem.n.01', 'trouble.n.01', 'grounds.n.03', 'yard.n.02', 'part.n.03', 'piece.n.01', 'reader.n.03', 'reader.n.01', 'year.n.01', 'year.n.02', 'southerly.s.02', 'southerly.s.01', 'death.n.02', 'death.n.04', 'sidewalk.n.01', 'pavement.n.01', 'evidence.n.01', 'evidence.n.02', 'empire.n.01', 'empire.n.02', 'effective.a.01', 'effective.s.03', 'administration.n.02', 'institution.n.01', 'exist.v.01', 'be.v.01', 'rational.a.01', 'intellectual.s.01', 'spirit.n.01', 'spirit.n.02', 'manner.n.01', 'manner.n.02', 'hurt.v.02', 'pain.v.02', 'bear.v.11', 'have.v.01', 'take.v.20', 'fill.v.04', 'statement.n.01', 'statement.n.04', 'rule.n.04', 'convention.n.02', 'spill_the_beans.v.01', 'spill.v.05', 'spirit.n.01', 'spirit.n.02', 'understand.v.02', 'see.v.05', 'detergent.n.01', 'detergent.n.02', 'listen.v.02', 'heed.v.01', 'be.v.01', 'constitute.v.01', 'hope.v.03', 'hope.v.01', 'lie.v.02', 'lie.v.01', 'come.v.03', 'arrive.v.01', 'administer.v.01', 'distribute.v.01', 'control.n.01', 'control.n.02', 'rule.n.03', 'rule.n.01', 'box.n.03', 'box.n.01', 'colorful.a.02', 'colorful.a.01', 'major.a.03', 'major.a.01', 'man.n.03', 'man.n.01', 'be.v.01', 'be.v.03', 'voyage.n.02', 'ocean_trip.n.01', 'year.n.01', 'year.n.02', 'trace.v.02', 'describe.v.01', 'expression.n.02', 'expression.n.03', 'act.v.01', 'act.v.04', 'job.n.06', 'job.n.02', 'target.n.04', 'prey.n.01', 'learn.v.04', 'read.v.03', 'target.n.04', 'prey.n.01', 'run.v.14', 'prevail.v.03', 'clear.s.02', 'clear.a.01', 'positive.a.01', 'plus.s.02', 'crisp.s.01', 'acuate.s.01', 'tone.n.02', 'timbre.n.01', 'consonant.n.01', 'consonant.n.02', 'keep.v.01', 'bear.v.11', 'keep.v.01', 'hold.v.02', 'concern.n.04', 'concern.n.02', 'analyze.v.01', 'probe.v.01', 'grant.n.02', 'grant.n.01', 'play.n.01', 'play.n.02', 'spirit.n.01', 'spirit.n.02', 'name.v.01', 'call.v.02', 'evolve.v.01', 'develop.v.01', 'restrain.v.03', 'hold.v.02', 'protective.a.01', 'protective.s.02', 'concern.n.02', 'concern.n.03', 'troop.n.02', 'troop.n.01', 'absurd.s.02', 'absurd.s.01', 'character.n.05', 'type.n.01', 'delicate.a.01', 'delicate.s.02', 'dim-witted.s.01', 'childlike.s.02', 'musician.n.01', 'musician.n.02', 'clear.s.02', 'clear.a.01', 'visit.v.01', 'see.v.01', 'defense.n.02', 'defense.n.01', 'spirit.n.01', 'spirit.n.02', 'detergent.n.01', 'detergent.n.02', 'event.n.02', 'case.n.01', 'lead.v.03', 'contribute.v.03', 'meet.v.08', 'meet.v.01', 'prize.n.01', 'award.n.02', 'agitation.n.04', 'excitation.n.03', 'keep.v.01', 'hold.v.02', 'rage.n.02', 'fury.n.01', 'study.n.04', 'survey.n.01', 'have.v.01', 'hold.v.02', 'go.v.02', 'travel.v.01', 'evolve.v.01', 'develop.v.01', 'reader.n.03', 'reader.n.01', 'valuable.a.01', 'valuable.s.02', 'deal.v.06', 'sell.v.01', 'survey.n.01', 'report.n.01', 'spirit.n.01', 'spirit.n.02', 'gun.n.01', 'artillery.n.01', 'maneuver.n.03', 'play.n.03', 'reward.n.02', 'reward.n.04', 'spirit.n.01', 'spirit.n.02', 'role.n.04', 'character.n.04', 'problem.n.02', 'problem.n.01', 'hold.v.10', 'hold.v.02', 'get.v.01', 'experience.v.03', 'visit.v.01', 'see.v.01', 'act.v.01', 'act.v.04', 'light.n.01', 'light.n.05', 'tone.n.02', 'timbre.n.01', 'travel.v.01', 'move.v.03', 'manage.v.02', 'treat.v.01', 'domain.n.02', 'country.n.02', 'spirit.n.01', 'spirit.n.02', 'delicate.s.03', 'delicate.a.01', 'little.s.03', 'small.a.01', 'escape.n.01', 'escape.n.02', 'troop.n.02', 'troop.n.01', 'keep.v.01', 'hold.v.02', 'shape.n.01', 'human_body.n.01', 'dress.v.04', 'dress.v.01', 'theology.n.01', 'theology.n.02', 'be.v.01', 'constitute.v.01', 'practice.n.01', 'practice.n.05', 'see.v.10', 'learn.v.02', 'solid.s.01', 'solid.s.07', 'spirit.n.01', 'spirit.n.02', 'prime_minister.n.01', 'chancellor.n.02', 'father.n.03', 'father.n.06', 'mountainous.s.03', 'cragged.s.01', 'understand.v.02', 'see.v.05', 'invite.v.07', 'request.v.01', 'product.n.02', 'merchandise.n.01', 'country.n.02', 'state.n.04', 'tone.n.02', 'timbre.n.01', 'fight.v.02', 'contend.v.06', 'crusade.v.01', 'physical.a.01', 'physical.s.05', 'problem.n.02', 'problem.n.01', 'change.n.03', 'change.n.01', 'go.v.02', 'go.v.03', 'agency.n.02', 'agency.n.01', 'contribution.n.01', 'part.n.01', 'resource.n.02', 'resource.n.01', 'reach.v.01', 'reach.v.07', 'gun.n.01', 'artillery.n.01', 'spirit.n.01', 'spirit.n.02', 'restlessness.n.02', 'impatience.n.02', 'control.n.01', 'control.n.05', 'control.n.01', 'control.n.02', 'remember.v.02', 'think_of.v.04', 'be.v.03', 'be.v.05', 'lead.v.03', 'contribute.v.03', 'tough.a.01', 'sturdy.s.03', 'full.s.06', 'good.a.01', 'do.v.03', 'make.v.01', 'flicker.n.01', 'sparkle.n.01', 'explode.v.04', 'explode.v.03', 'base.s.05', 'hateful.s.02', 'depart.v.03', 'take_off.v.03', 'allege.v.01', 'state.v.01', 'change.n.02', 'change.n.04', 'problem.n.02', 'problem.n.01', 'end.n.05', 'end.n.03', 'see.v.12', 'see.v.15', 'miles_per_hour.n.01', 'miles_per_hour.n.02', 'lead.v.03', 'contribute.v.03', 'be.v.02', 'be.v.08', 'mistake.n.01', 'erroneousness.n.01', 'tell.v.03', 'state.v.01', 'break.v.18', 'break.v.10', 'feel.v.03', 'feel.v.01', 'deduction.n.04', 'deduction.n.03', 'grant.n.02', 'grant.n.01', 'direction.n.03', 'direction.n.01', 'deviation.n.01', 'difference.n.01', 'let_go.v.02', 'let_go_of.v.01', 'control.n.01', 'restraint.n.02', 'remove.v.01', 'take.v.08', 'correct.s.02', 'right.a.04', 'problem.n.02', 'problem.n.01', 'keep.v.01', 'hold.v.02', 'suffer.v.02', 'suffer.v.08', 'extinguish.v.04', 'obviate.v.01', 'shape.v.03', 'shape.v.02', 'feel.v.06', 'feel.v.01', 'emission.n.01', 'discharge.n.03', 'conference.n.03', 'conference.n.01', 'text.n.01', 'text.n.04', 'text.n.01', 'text.n.04', 'question.n.01', 'question.n.02', 'representative.n.01', 'spokesperson.n.01', 'text.n.01', 'text.n.04', 'emission.n.01', 'discharge.n.03', 'emission.n.01', 'discharge.n.03', 'envoy.n.01', 'emissary.n.01', 'emission.n.01', 'discharge.n.03', 'text.n.01', 'text.n.04', 'urgency.n.02', 'urgency.n.01', 'hall.n.03', 'hallway.n.01', 'anteroom.n.01', 'claim.n.03', 'claim.n.01', 'performance.n.03', 'performance.n.04', 'absence.n.02', 'absence.n.01', 'end.n.05', 'end.n.03', 'end.n.02', 'end.n.05', 'end.n.03', 'end.n.02', 'return.n.05', 'return.n.02', 'end.n.05', 'end.n.03', 'end.n.02', 'end.n.05', 'end.n.03', 'end.n.02', 'analyst.n.01', 'analyst.n.02', 'confidence.n.02', 'confidence.n.03', 'advance.n.06', 'rise.n.01', 'treasury.n.02', 'department_of_the_treasury.n.01', 'instability.n.04', 'instability.n.02', 'instability.n.01', 'war.n.01', 'war.n.02', 'analyst.n.01', 'analyst.n.02', 'war.n.01', 'war.n.02', 'news.n.01', 'news.n.02', 'security.n.09', 'security.n.01', 'instability.n.04', 'instability.n.02', 'instability.n.01', 'analyst.n.01', 'analyst.n.02', 'output.n.02', 'output.n.05', 'output.n.04', 'state.n.04', 'state.n.03', 'investing.n.01', 'investment.n.02', 'increase.n.04', 'increase.n.02', 'end.n.03', 'end.n.02', 'risk.n.02', 'danger.n.01', 'question.n.01', 'question.n.02', 'argumentation.n.02', 'argument.n.01', 'means.n.01', 'manner.n.01', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'reaction.n.03', 'reaction.n.02', 'reaction.n.05', 'argumentation.n.02', 'argument.n.01', 'news.n.02', 'news_program.n.01', 'news.n.01', 'news.n.02', 'news.n.04', 'position.n.03', 'opinion.n.01', 'opinion.n.02', 'argumentation.n.02', 'argument.n.01', 'change.n.03', 'change.n.01', 'change.n.02', 'end.n.05', 'end.n.03', 'end.n.02', 'means.n.01', 'manner.n.01', 'circumstance.n.01', 'context.n.02', 'organization.n.01', 'administration.n.02', 'question.n.01', 'question.n.02', 'question.n.03', 'situation.n.01', 'situation.n.02', 'advantage.n.01', 'advantage.n.03', 'advantage.n.01', 'advantage.n.03', 'degree.n.01', 'degree.n.02', 'means.n.01', 'manner.n.01', 'search.n.01', 'search.n.02', 'discovery.n.03', 'discovery.n.02', 'planet.n.01', 'planet.n.03', 'means.n.01', 'manner.n.01', 'support.n.02', 'support.n.03', 'state.n.04', 'country.n.02', 'change.n.04', 'change.n.02', 'temper.n.02', 'climate.n.02', 'support.n.02', 'support.n.03', 'state.n.04', 'country.n.02', 'support.n.02', 'support.n.03', 'output.n.02', 'output.n.05', 'output.n.04', 'state.n.04', 'country.n.02', 'increase.n.04', 'increase.n.02', 'confidence.n.02', 'confidence.n.03', 'support.n.02', 'support.n.03', 'temper.n.02', 'climate.n.02', 'trouble.n.01', 'problem.n.01', 'support.n.02', 'support.n.03', 'development.n.01', 'development.n.02', 'development.n.07', 'position.n.03', 'opinion.n.01', 'leadership.n.01', 'leadership.n.04', 'leadership.n.03', 'conference.n.03', 'conference.n.01', 'inspection.n.01', 'review.n.08', 'reappraisal.n.01', 'conference.n.03', 'conference.n.01', 'organization.n.01', 'administration.n.02', 'year.n.01', 'year.n.02', 'topic.n.02', 'subject.n.01', 'trouble.n.01', 'problem.n.01', 'state.n.04', 'country.n.02', 'degree.n.01', 'degree.n.02', 'degree.n.01', 'degree.n.02', 'trouble.n.01', 'problem.n.01', 'trouble.n.01', 'problem.n.01', 'employment.n.02', 'employment.n.03', 'employment.n.01', 'state.n.04', 'country.n.02', 'employment.n.02', 'employment.n.03', 'employment.n.01', 'substitute.n.01', 'surrogate.n.01', 'employment.n.02', 'employment.n.01', 'means.n.01', 'manner.n.01', 'state.n.04', 'country.n.02', 'increase.n.04', 'increase.n.02', 'organization.n.01', 'administration.n.02', 'trouble.n.01', 'trouble.n.03', 'worry.n.02', 'trouble.n.01', 'problem.n.01', 'new_york.n.02', 'new_york.n.01', 'claim.n.03', 'claim.n.01', 'claim.n.03', 'claim.n.01', 'claim.n.03', 'claim.n.01', 'claim.n.03', 'claim.n.01', 'explain.v.01', 'explain.v.02', 'use.n.01', 'use.n.03', 'reach.v.07', 'achieve.v.01', 'want.v.02', 'need.v.03', 'information.n.02', 'data.n.01', 'part.n.09', 'part.n.01', 'information.n.02', 'data.n.01', 'part.n.09', 'part.n.01', 'powder.n.03', 'powder.n.01', 'region.n.01', 'part.n.03', 'part.n.01', 'reduce.v.01', 'reduce.v.11', 'reduce.v.08', 'have.v.11', 'experience.v.03', 'moderate.a.01', 'moderate.s.02', 'dangerous.s.02', 'severe.s.01', 'severe.s.06', 'trouble.n.01', 'problem.n.01', 'information.n.02', 'data.n.01', 'part.n.09', 'part.n.01', 'kill.v.01', 'kill.v.10', 'kill.v.09', 'produce.v.01', 'produce.v.03', 'testify.v.02', 'show.v.04', 'effective.a.01', 'effective.s.02', 'effective.s.03', 'median.s.01', 'average.s.01', 'modal.s.01', 'median.s.01', 'average.s.01', 'modal.s.01', 'simple.a.01', 'elementary.s.01', 'actually.r.02', 'actually.r.04', 'nowadays.r.01', 'now.r.04', 'perform.v.01', 'make.v.01', 'do.v.03', 'calculation.n.01', 'calculation.n.02', 'understand.v.01', 'understand.v.02', 'function.v.01', 'work.v.03', 'convert.v.01', 'change.v.06', 'convert.v.02', 'similar.a.01', 'like.a.01', 'alike.a.01', 'means.n.01', 'manner.n.01', 'means.n.01', 'manner.n.01', 'determine.v.01', 'detect.v.01', 'means.n.01', 'manner.n.01', 'insert.v.02', 'insert.v.01', 'determine.v.01', 'detect.v.01', 'have.v.01', 'have.v.02', 'means.n.01', 'manner.n.01', 'last.a.02', 'last.s.01', 'solution.n.02', 'result.n.03', 'begin.v.02', 'begin.v.06', 'solution.n.02', 'result.n.03', 'perform.v.01', 'do.v.03', 'typical.a.01', 'typical.s.03', 'color.n.01', 'color.n.08', 'organize.v.05', 'organize.v.02', 'deal.v.03', 'manage.v.02', 'organization.n.01', 'administration.n.02', 'planning.n.01', 'planning.n.02', 'determine.v.01', 'detect.v.01', 'common.a.02', 'common.s.04', 'diverseness.n.01', 'assortment.n.01', 'unlike.a.01', 'different.s.05', 'committee.n.01', 'deputation.n.01', 'major.a.02', 'major.a.01', 'publish.v.02', 'print.v.01', 'survey.n.01', 'report.n.01', 'part.n.09', 'part.n.01', 'personal.a.01', 'personal.s.02', 'representative.n.01', 'spokesperson.n.01', 'national.a.03', 'national.a.01', 'organization.n.01', 'administration.n.02', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'include.v.03', 'include.v.02', 'conference.n.03', 'conference.n.01', 'employment.n.02', 'employment.n.03', 'employment.n.01', 'committee.n.01', 'deputation.n.01', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'analyze.v.01', 'analyze.v.03', 'background.n.03', 'setting.n.02', 'employment.n.02', 'employment.n.03', 'employment.n.01', 'organization.n.01', 'administration.n.02', 'trouble.n.01', 'problem.n.01', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'publish.v.02', 'print.v.01', 'characteristic.n.02', 'feature.n.01', 'analyze.v.01', 'analyze.v.03', 'causal_agent.n.01', 'cause.n.01', 'give.v.10', 'give.v.05', 'remove.v.01', 'get_rid_of.v.01', 'old.s.04', 'aged.s.01', 'retirement.n.02', 'retirement.n.01', 'old.s.04', 'aged.s.01', 'refer.v.02', 'relate.v.04', 'explain.v.01', 'explain.v.02', 'reach.v.01', 'reach.v.07', 'achieve.v.01', 'have.v.01', 'have.v.02', 'side.n.05', 'side.n.04', 'side.n.05', 'side.n.04', 'side.n.05', 'side.n.04', 'mean.v.03', 'entail.v.01', 'special.s.04', 'particular.s.01', 'testify.v.02', 'show.v.04', 'prove.v.02', 'testify.v.02', 'effective.a.01', 'effective.s.02', 'effective.s.03', 'begin.v.03', 'begin.v.08', 'intact.s.04', 'integral.s.02', 'follow-up.n.03', 'follow-up.n.02', 'cause.n.02', 'cause.n.01', 'duration.n.03', 'duration.n.01', 'contact.n.02', 'contact.n.03', 'especial.s.01', 'particular.s.02', 'people.n.01', 'multitude.n.03', 'peculiar.s.04', 'particular.s.01', 'peculiarity.n.02', 'peculiarity.n.01', 'door.n.01', 'doorway.n.01', 'sound.v.02', 'sound.v.03', 'delight.v.02', 'enjoy.v.01', 'love.v.02', 'sound.v.02', 'sound.v.03', 'train.v.01', 'coach.v.01', 'prepare.v.05', 'dance.n.04', 'dance.n.02', 'youth.n.06', 'young.n.09', 'youth.n.04', 'youth.n.03', 'side.n.10', 'side.n.02', 'play.v.06', 'play.v.07', 'play.v.03', 'europe.n.03', 'europe.n.01', 'want.v.02', 'necessitate.v.01', 'mental.a.01', 'mental.a.02', 'assiduity.n.01', 'concentration.n.05', 'simple.a.01', 'elementary.s.01', 'signal.n.01', 'signal.n.02', 'sound.v.02', 'sound.v.03', 'version.n.02', 'variation.n.03', 'give.v.36', 'give.v.01', 'yield.v.01', 'hole.n.02', 'hole.n.01', 'watch.v.01', 'watch.v.04', 'watch.v.02', 'describe.v.01', 'report.v.01', 'receive.v.02', 'find.v.15', 'satisfaction.n.01', 'gratification.n.01', 'mechanical.a.01', 'mechanical.a.03', 'frequently.r.01', 'often.r.03', 'frequently.r.01', 'often.r.03', 'separate.a.01', 'freestanding.s.01', 'new.a.01', 'new.s.05', 'come.v.01', 'arrive.v.01', 'feeling.n.01', 'impression.n.01', 'topic.n.02', 'issue.n.01', 'draw_a_bead_on.v.02', 'aim.v.02', 'publish.v.02', 'print.v.01', 'state.n.04', 'country.n.02', 'care.n.06', 'care.n.01', 'care.n.05', 'trouble.n.01', 'problem.n.01', 'allege.v.01', 'read.v.02', 'train.v.01', 'coach.v.01', 'prepare.v.05', 'allege.v.01', 'say.v.09', 'new.a.01', 'raw.s.12', 'new.s.05', 'enter.v.02', 'enter.v.01', 'discipline.n.01', 'sphere.n.01', 'state.n.04', 'country.n.02', 'establish.v.02', 'establish.v.01', 'trouble.n.01', 'problem.n.01', 'function.n.03', 'role.n.04', 'meeting.n.01', 'meeting.n.02', 'postdate.v.01', 'follow.v.06', 'follow.v.03', 'ill.r.01', 'badly.r.07', 'appareled.s.01', 'dressed.s.04', 'see.v.01', 'witness.v.02', 'growth.n.07', 'growth.n.01', 'allege.v.01', 'say.v.09', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'future.a.01', 'future.s.02', 'discovery.n.01', 'discovery.n.03', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'know.v.01', 'know.v.03', 'then.r.01', 'then.r.02', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'make.v.03', 'cause.v.01', 'function.v.01', 'work.v.03', 'turn.v.07', 'become.v.02', 'discover.v.04', 'identify.v.05', 'discover.v.03', 'detect.v.01', 'make.v.03', 'cause.v.01', 'life.n.05', 'life.n.06', 'develop.v.10', 'originate.v.01', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'know.v.01', 'know.v.03', 'know.v.01', 'know.v.03', 'suffer.v.01', 'suffer.v.10', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'genetic.a.04', 'genic.a.01', 'prove.v.02', 'testify.v.02', 'little.s.08', 'little.s.03', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'prove.v.01', 'turn_out.v.02', 'cardinal.s.01', 'fundamental.s.03', 'insight.n.03', 'penetration.n.02', 'basic.a.01', 'basic.s.03', 'result.n.03', 'consequence.n.01', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'evidence.n.01', 'evidence.n.02', 'common.a.02', 'common.s.04', 'right.r.01', 'right.r.02', 'look.v.02', 'appear.v.04', 'all-important.s.01', 'crucial.s.02', 'evidence.n.01', 'evidence.n.02', 'develop.v.10', 'originate.v.01', 'means.n.01', 'manner.n.01', 'means.n.01', 'manner.n.01', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'bit.n.02', 'spot.n.10', 'piece.n.01', 'part.n.03', 'loss.n.04', 'loss.n.01', 'discover.v.04', 'determine.v.01', 'find.v.10', 'analyze.v.01', 'analyze.v.02', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'prove.v.02', 'testify.v.02', 'loss.n.04', 'loss.n.01', 'contribute.v.03', 'lead.v.03', 'outbreak.n.01', 'eruption.n.02', 'loss.n.04', 'loss.n.01', 'large.a.01', 'big.s.05', 'frequently.r.01', 'often.r.03', 'degree.n.02', 'phase.n.01', 'frequently.r.01', 'often.r.03', 'degree.n.02', 'phase.n.01', 'dangerous.s.02', 'severe.s.01', 'severe.s.06', 'search.v.01', 'search.v.02', 'diverseness.n.01', 'assortment.n.01', 'develop.v.10', 'originate.v.01', 'develop.v.10', 'originate.v.01', 'evidence.n.01', 'evidence.n.02', 'loss.n.04', 'loss.n.01', 'common.a.02', 'common.s.04', 'bit.n.02', 'spot.n.10', 'piece.n.01', 'part.n.03', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'doubtful.s.02', 'dubious.s.03', 'search.n.01', 'search.n.02', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'growth.n.01', 'growth.n.06', 'growth.n.01', 'growth.n.06', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'prove.v.01', 'turn_out.v.02', 'discovery.n.01', 'discovery.n.03', 'discovery.n.02', 'abruptly.r.01', 'suddenly.r.01', 'right.r.01', 'right.r.03', 'evidence.n.01', 'evidence.n.02', 'nowadays.r.01', 'now.r.04', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'allege.v.01', 'say.v.09', 'frequently.r.01', 'often.r.03', 'close.a.01', 'near.a.01', 'discover.v.04', 'determine.v.01', 'discover.v.03', 'detect.v.01', 'common.a.02', 'common.s.04', 'discovery.n.01', 'discovery.n.03', 'discovery.n.02', 'speculate.v.01', 'speculate.v.02', 'new.a.01', 'fresh.s.04', 'new.s.05', 'new.s.04', 'allege.v.01', 'say.v.09', 'new.a.01', 'new.a.06', 'fresh.s.04', 'new.s.04', 'growth.n.01', 'growth.n.06', 'produce.v.01', 'produce.v.03', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'effective.a.01', 'effective.s.02', 'command.v.02', 'ask.v.04', 'institution.n.01', 'administration.n.02', 'politician.n.02', 'politician.n.01', 'sample.n.01', 'sample_distribution.n.01', 'adolescent.a.01', 'adolescent.s.04', 'degree.n.01', 'degree.n.02', 'let.v.01', 'leave.v.06', 'education.n.01', 'education.n.04', 'institution.n.01', 'administration.n.02', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'simple.a.01', 'elementary.s.01', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'profession.n.02', 'profession.n.01', 'think.v.02', 'think.v.05', 'institution.n.01', 'administration.n.02', 'teaching.n.01', 'education.n.01', 'truly.r.01', 'in_truth.r.01', 'serve.v.08', 'serve.v.01', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'component.n.01', 'factor.n.01', 'trouble.n.01', 'problem.n.01', 'trouble.n.01', 'problem.n.01', 'circulate.v.02', 'spread.v.01', 'politician.n.02', 'politician.n.01', 'assume.v.05', 'come_to.v.03', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'evidence.n.01', 'evidence.n.02', 'evidence.n.01', 'evidence.n.02', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'poor.a.02', 'poor.a.03', 'know.v.01', 'know.v.03', 'know.v.10', 'concerned.a.01', 'concerned.s.02', 'conventional.a.01', 'conventional.a.04', 'conventional.s.02', 'conference.n.03', 'conference.n.01', 'state.n.04', 'nation.n.02', 'development.n.01', 'development.n.02', 'need.n.02', 'need.n.01', 'know.v.01', 'know.v.03', 'early.a.01', 'early.a.02', 'performance.n.03', 'performance.n.04', 'blame.v.03', 'blame.v.01', 'corrupt.a.01', 'crooked.a.02', 'politician.n.02', 'politician.n.01', 'politician.n.02', 'politician.n.01', 'circumstance.n.01', 'context.n.02', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'simple.a.01', 'elementary.s.01', 'education.n.01', 'education.n.03', 'performance.n.03', 'performance.n.04', 'student.n.01', 'schoolchild.n.01', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'reason.n.01', 'new.a.01', 'new.s.05', 'money.n.01', 'money.n.02', 'nowadays.r.01', 'now.r.04', 'student.n.01', 'schoolchild.n.01', 'performance.n.03', 'performance.n.04', 'means.n.01', 'manner.n.01', 'function.n.02', 'purpose.n.01', 'enter.v.02', 'enter.v.01', 'figure.v.02', 'function.n.02', 'purpose.n.01', 'encourage.v.03', 'encourage.v.02', 'full.s.03', 'entire.s.01', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'important.a.01', 'crucial.a.01', 'significant.a.01', 'school.n.01', 'school.n.06', 'outside.s.03', 'external.s.02', 'powerful.a.01', 'potent.s.01', 'means.n.01', 'manner.n.01', 'performance.n.03', 'performance.n.04', 'think.v.01', 'think.v.02', 'think.v.08', 'second.n.01', 'moment.n.02', 'visualize.v.01', 'see.v.01', 'induce.v.02', 'make.v.08', 'sit.v.01', 'sit_down.v.01', 'have.v.01', 'have.v.10', 'feel.v.04', 'feel.v.05', 'feel.v.01', 'perform.v.01', 'make.v.01', 'do.v.03', 'try.v.07', 'test.v.01', 'feel.v.04', 'feel.v.05', 'feel.v.01', 'allege.v.01', 'say.v.09', 'time.n.05', 'time.n.02', 'trouble.n.04', 'trouble.n.01', 'trouble.n.03', 'watch.v.01', 'watch.v.04', 'watch.v.02', 'feel.v.04', 'feel.v.05', 'feel.v.01', 'learn.v.02', 'determine.v.08', 'see.v.01', 'stay.v.02', 'bide.v.01', 'stay.v.04', 'short.a.03', 'small.a.01', 'mind.n.01', 'mind.n.02', 'mind.n.05', 'wish.v.03', 'wish.v.01', 'perform.v.01', 'make.v.01', 'do.v.03', 'pavement.n.01', 'sidewalk.n.01', 'feel.v.04', 'feel.v.01', 'family.n.02', 'family.n.01', 'suffer.v.06', 'suffer.v.03', 'man.n.01', 'man.n.03', 'know.v.01', 'know.v.03', 'know.v.01', 'know.v.03', 'sit.v.01', 'sit_down.v.01', 'expansive.s.04', 'chatty.s.01', 'delight.v.02', 'enjoy.v.01', 'love.v.02', 'real.a.02', 'actual.s.03', 'end.n.05', 'end.n.02', 'make.v.03', 'cause.v.01', 'apparition.n.03', 'shadow.n.01', 'make.v.03', 'cause.v.01', 'apparition.n.03', 'shadow.n.01', 'shrink.v.04', 'flinch.v.01', 'trap.v.02', 'trap.v.04', 'trap.v.01', 'short.a.03', 'small.a.01', 'wish.v.03', 'wish.v.01', 'crossing.n.05', 'intersection.n.02', 'means.n.01', 'manner.n.01', 'means.n.01', 'manner.n.01', 'expression.n.01', 'face.n.01', 'trouble.n.04', 'trouble.n.01', 'trouble.n.03', 'fuss.n.02', 'trouble.n.04', 'trouble.n.01', 'trouble.n.03', 'fuss.n.02', 'disturb.v.01', 'upset.v.02', 'interrupt.v.04', 'break.v.24', 'topic.n.02', 'subject.n.01', 'alert.a.01', 'alert.s.03', 'allege.v.01', 'say.v.09', 'have.v.01', 'own.v.01', 'come.v.01', 'arrive.v.01', 'tell.v.02', 'state.v.01', 'feel.v.04', 'feel.v.01', 'perform.v.01', 'make.v.01', 'do.v.03', 'feel.v.04', 'feel.v.01', 'clever.s.03', 'crafty.s.01', 'means.n.01', 'manner.n.01', 'permit.v.01', 'let.v.01', 'major.a.03', 'major.a.01', 'simple.a.01', 'elementary.s.01', 'assumption.n.02', 'premise.n.01', 'political.a.01', 'political.a.03', 'state.n.04', 'state.n.03', 'have.v.01', 'have.v.10', 'vote.v.02', 'vote.v.03', 'vote.v.05', 'vote.v.01', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'state.n.04', 'state.n.03', 'assumption.n.02', 'premise.n.01', 'have.v.01', 'have.v.10', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'effective.a.01', 'effective.s.02', 'individual.a.01', 'individual.s.02', 'simple.a.01', 'elementary.s.01', 'trouble.n.01', 'problem.n.01', 'individual.a.01', 'individual.s.02', 'act.v.02', 'do.v.03', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'have.v.11', 'have.v.01', 'have.v.02', 'have.v.10', 'vehemence.n.01', 'emphasis.n.01', 'function.n.03', 'role.n.04', 'circulate.v.02', 'spread.v.01', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'understand.v.02', 'recognize.v.02', 'reason.n.06', 'cause.n.02', 'reason.n.02', 'have.v.01', 'have.v.10', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'evidence.n.01', 'evidence.n.02', 'assumption.n.02', 'premise.n.01', 'discover.v.04', 'determine.v.01', 'detect.v.01', 'personal.a.01', 'personal.s.02', 'primary.a.01', 'chief.s.01', 'personal.a.01', 'personal.s.02', 'bespeak.v.01', 'argue.v.03', 'assumption.n.02', 'premise.n.01', 'have.v.11', 'have.v.01', 'have.v.02', 'have.v.10', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'bespeak.v.01', 'argue.v.03', 'testify.v.02', 'show.v.10', 'picture.v.02', 'personal.a.01', 'personal.s.02', 'national.a.02', 'national.a.03', 'national.a.01', 'home.s.03', 'change.n.03', 'change.n.01', 'change.n.04', 'change.n.02', 'personal.a.01', 'personal.s.02', 'control.n.05', 'control.n.01', 'control.n.02', 'personal.a.01', 'personal.s.02', 'evidence.n.01', 'evidence.n.02', 'incumbency.n.03', 'tenure.n.01', 'prove.v.01', 'turn_out.v.02', 'severe.s.01', 'severe.s.06', 'major.a.03', 'major.a.01', 'state.n.04', 'state.n.03', 'search.v.01', 'search.v.02', 'comfort.n.02', 'comfort.n.01', 'comfort.n.05', 'consolation.n.01', 'solace.n.02', 'vivid.s.02', 'graphic.s.05', 'begin.v.02', 'begin.v.06', 'evanesce.v.01', 'elapse.v.01', 'shake.v.02', 'shake.v.01', 'judder.v.01', 'rock.v.01', 'toss.v.06', 'chuck.v.01', 'library.n.03', 'library.n.01', 'library.n.05', 'heave.v.07', 'buckle.v.02', 'smoke.n.01', 'smoke.n.02', 'air.n.02', 'air.n.01', 'shake.v.02', 'shake.v.01', 'judder.v.01', 'rock.v.01', 'liquefy.v.03', 'liquefy.v.01', 'watch.v.01', 'watch.v.04', 'watch.v.02', 'means.n.01', 'manner.n.01', 'part.n.02', 'region.n.01', 'part.n.01', 'snap.v.03', 'crack.v.01', 'snap.v.06', 'crack.v.02', 'olfactory_property.n.01', 'smell.n.01', 'shake.v.02', 'shake.v.01', 'judder.v.01', 'rock.v.01', 'shake.v.02', 'shake.v.01', 'judder.v.01', 'rock.v.01', 'part.n.09', 'region.n.01', 'part.n.01', 'batch.n.02', 'mess.n.01', 'unpredictable.a.01', 'unpredictable.s.02', 'disquieted.s.01', 'stressed.s.01', 'wheel.v.03', 'roll.v.11', 'matter.n.01', 'thing.n.05', 'door.n.01', 'doorway.n.01', 'watch.v.01', 'watch.v.04', 'watch.v.02', 'look.v.03', 'look.v.02', 'light.n.02', 'light.n.01', 'friend.n.01', 'acquaintance.n.03', 'understand.v.02', 'recognize.v.02', 'get.v.03', 'permit.v.01', 'let.v.01', 'come.v.01', 'arrive.v.01', 'stand.v.01', 'stand.v.03', 'watch.v.01', 'watch.v.04', 'watch.v.02', 'stand.v.01', 'stand.v.03', 'life.n.03', 'life.n.06', 'see.v.01', 'watch.v.03', 'picture.n.01', 'photograph.n.01', 'meet.v.09', 'meet.v.02', 'new.a.01', 'new.s.05', 'shake.v.02', 'shake.v.01', 'judder.v.01', 'rock.v.01', 'tape.n.02', 'magnetic_tape.n.01', 'stand.v.01', 'stand.v.03', 'emerge.v.01', 'issue.v.04', 'emerge.v.04', 'survive.v.01', 'live.v.02']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "print(len(multi_label_list))\n",
        "print(multi_label_list)\n",
        "lengths = [len(l) for l in multi_label_list]\n",
        "c = Counter(lengths)\n",
        "print(c)\n",
        "\n",
        "fine_id2sense = json.load(open(\"data/mapping/fine_id2sense.json\", \"r\"))\n",
        "multi_label_list = [fine_id2sense[str(e)] for l in multi_label_list for e in l]\n",
        "print(len(multi_label_list))\n",
        "print(multi_label_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFGbs4_Zl8w"
      },
      "source": [
        "### Preprocessing\n",
        "[do not necessarily use it]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmMJ-O2Zl8x"
      },
      "source": [
        "#### Clean tokens"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUBPDkLZl8x"
      },
      "source": [
        "With respect to the first homework the *cleaning* operations (also due the power of *BERT Tokenizer*) are very basic and not \"aggressive\".\n",
        "\n",
        "> 🔸 The function I implemented is \"*clean_tokens*\" from the *data_module.py* file. Of course, this function is applied to all the dataset splits (*train/val/test*)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfV9bgkZl8x"
      },
      "source": [
        "#### Filter sentences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jd3TTvLZZl8x"
      },
      "source": [
        "Another important step before finishing the preprocessing part, is to filter out the *training* sentences. This is something it has to be done only at training time because the test/val datasets don't have to be touched in this sense. <br> Let's first see which is the histogram of sentences length."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BaynqYLrbhu9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C47XCTYUZl82"
      },
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaE5pz6fZl82"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"coarse\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BGMMZ7pSZl82"
      },
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ca5ZHIZl83"
      },
      "outputs": [],
      "source": [
        "wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "version_name = \"fine\"\n",
        "with wandb.init(entity=\"lavallone\", project=\"homonyms\", name=version_name, mode=\"online\"):\n",
        "    hparams = asdict(Hparams())\n",
        "\n",
        "    data = WSD_DataModule(hparams)\n",
        "    model = WSD_Model(hparams)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "    \n",
        "    train_model(data, model, experiment_name=version_name, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SwNRcdPbjvg"
      },
      "source": [
        "### Hparams tuning\n",
        "[if needed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fns1bRRPZl83"
      },
      "outputs": [],
      "source": [
        "# def training_pipeline(config=None):\n",
        "#     hparams_tuning = False\n",
        "#     version_name = \"BASELINE\"\n",
        "#     with wandb.init(entity=\"lavallone\", project=\"NLP\", name=version_name, mode=\"online\", config=config):\n",
        "#         seed = wandb.config.seed if hparams_tuning else 1999\n",
        "#         set_seed(seed)\n",
        "#         hparams = asdict(Hparams())\n",
        "#         # when doing the hparams search, this is how each run we change them to search for the best combinations!\n",
        "#         if hparams_tuning:\n",
        "#             hparams[\"batch_size\"] = wandb.config.batch_size\n",
        "#             hparams[\"dropout\"] = wandb.config.dropout\n",
        "#             hparams[\"lr\"] = wandb.config.lr\n",
        "#             hparams[\"hidden_dim\"] = wandb.config.hidden_dim\n",
        "\n",
        "#         data = WSD_DataModule(hparams)\n",
        "#         model = WSD_Model(hparams,\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine2coarse.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_fine_defs_map.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_id2sense.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/fine_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_sense2id.json\", \"r\")),\n",
        "#                           json.load(open(hparams[\"prefix_path\"]+\"model/files/coarse_id2sense.json\", \"r\"))\n",
        "#                           )\n",
        "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#         model.to(device)\n",
        "        \n",
        "#         train_model(data, model, experiment_name=version_name, patience=5, metric_to_monitor=\"val_loss\", mode=\"min\", epochs=100, precision=hparams[\"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAV15D1bl14"
      },
      "outputs": [],
      "source": [
        "# wandb.login() # this is the key to paste each time for login: 65a23b5182ca8ce3eb72530af592cf3bfa19de85\n",
        "\n",
        "# sweep_config = {'method': 'random',\n",
        "#                 'metric': {'goal': 'maximize', 'name': 'val_micro_f1', 'target' : 0.89},\n",
        "#                 'parameters': {\n",
        "#                                 'batch_size': {'values': [64, 128, 256, 512]},\n",
        "#                                 'dropout': {'distribution': 'uniform', 'min': 0.3, 'max': 0.5},\n",
        "#                                 'lr': {'distribution': 'uniform', 'min': 1e-5, 'max': 1e-2},\n",
        "#                                 'hidden_dim': {'distribution': 'int_uniform', 'min': 200, 'max': 600},\n",
        "#                             }\n",
        "#                }\n",
        "\n",
        "# sweep_id = wandb.sweep(sweep=sweep_config, project=\"NLP\", entity=\"lavallone\")\n",
        "# wandb.agent(sweep_id, function=training_pipeline, count=20)\n",
        "# wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4txjgQRtGs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COARSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "base_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE2CLUSTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fine_ckpt = \"checkpoints/fine.ckpt\" \n",
        "\n",
        "model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on homonym clusters using a fine-grained model\n",
        "fine2cluster_evaluation(model, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLUSTER FILTERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_coarse_ckpt = \"checkpoints/coarse.ckpt\" \n",
        "best_fine_ckpt = \"checkpoints/fine.ckpt\"\n",
        "\n",
        "coarse_model = WSD_Model.load_from_checkpoint(best_coarse_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fine_model = WSD_Model.load_from_checkpoint(best_fine_ckpt, strict=False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data = WSD_DataModule(coarse_model.hparams)\n",
        "data.setup()\n",
        "\n",
        "# evaluation on fine senses using a coarse model for filtering out\n",
        "cluster_filter_evaluation(coarse_model, fine_model, data, oracle_or_not=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dzLDtSF5Zl8o",
        "ky0iP4FZZl8q",
        "KqwcyAGAZl8t",
        "lVFGbs4_Zl8w",
        "5FcrCHdjQ9Jf",
        "cTH0L6tvZl8z",
        "_fybkSIAZl8z",
        "ZEzTn9RuZl81",
        "IFPNDG-4Zl81",
        "BaynqYLrbhu9",
        "_SwNRcdPbjvg",
        "ue4txjgQRtGs",
        "Q3V_DSDzZl86"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
